{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyter-n.mekkes@gmail.com-f6d87/.conda/envs/temporal5/lib/python3.6/site-packages/scipy/__init__.py:147: UserWarning: NumPy 1.14.5 or above is required for this version of SciPy (detected version 1.14.0)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from data_handler import DataHandler\n",
    "from models import create_grud_model, load_grud_model\n",
    "from nn_utils.callbacks import ModelCheckpointwithBestWeights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set GPU usage for tensorflow backend\n",
    "if K.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = .1\n",
    "    config.gpu_options.allow_growth = True\n",
    "    K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clinical_history_5_observations\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "save_name = f'clinical_history_{n}_observations'\n",
    "print(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(batch_size=32, dataset_name='clinical_history_5_observations', early_stopping_patience=10, epochs=50, hidden_dim=[], label_name='taskname', max_timestamp=None, max_timesteps=200, model='GRUD', pretrained_model_file=None, recurrent_dim=[64], use_bidirectional_rnn=False, working_path='.')\n"
     ]
    }
   ],
   "source": [
    "# parse arguments\n",
    "## general\n",
    "arg_parser = argparse.ArgumentParser()\n",
    "arg_parser.add_argument('--working_path', default='.')\n",
    "\n",
    "## data\n",
    "arg_parser.add_argument('dataset_name', default='mimic3',\n",
    "                        help='The data files should be saved in [working_path]/data/[dataset_name] directory.')\n",
    "arg_parser.add_argument('label_name', default='mortality')\n",
    "arg_parser.add_argument('--max_timesteps', type=int, default=200, \n",
    "                        help='Time series of at most # time steps are used. Default: 200.')\n",
    "arg_parser.add_argument('--max_timestamp', default=None,# changed by nienke, was 48*60*60,, type=int\n",
    "                        help='Time series of at most # seconds are used. Default: 48 (hours).')\n",
    "\n",
    "## model\n",
    "arg_parser.add_argument('--recurrent_dim', type=lambda x: x and [int(xx) for xx in x.split(',')] or [], default='64')\n",
    "arg_parser.add_argument('--hidden_dim', type=lambda x: x and [int(xx) for xx in x.split(',')] or [], default='64')\n",
    "arg_parser.add_argument('--model', default='GRUD', choices=['GRUD', 'GRUforward', 'GRU0', 'GRUsimple'])\n",
    "arg_parser.add_argument('--use_bidirectional_rnn', default=False)\n",
    "                           \n",
    "## training\n",
    "arg_parser.add_argument('--pretrained_model_file', default=None,\n",
    "                        help='If pre-trained model is provided, training will be skipped.') # e.g., [model_name]_[i_fold].h5\n",
    "arg_parser.add_argument('--epochs', type=int, default=100)\n",
    "arg_parser.add_argument('--early_stopping_patience', type=int, default=10)\n",
    "arg_parser.add_argument('--batch_size', type=int, default=32)\n",
    "\n",
    "\n",
    "## set the actual arguments if running in notebook\n",
    "if not (__name__ == '__main__' and '__file__' in globals()):\n",
    "    ARGS = arg_parser.parse_args([\n",
    "        save_name,# change nienke (was sample) \n",
    "        'taskname',\n",
    "        '--model', 'GRUD',\n",
    "        '--hidden_dim', '',\n",
    "        '--epochs', '50', # change nienke (was 100)\n",
    "        '--batch_size', '32' #change nienke (was 32)\n",
    "#         '--max_timestamp', 'None' # changed by nienke\n",
    "    ])\n",
    "else:\n",
    "    ARGS = arg_parser.parse_args()\n",
    "\n",
    "print('Arguments:', ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "dataset = DataHandler(\n",
    "    data_path=os.path.join(ARGS.working_path, 'data', ARGS.dataset_name), \n",
    "    label_name=ARGS.label_name, \n",
    "    max_steps=ARGS.max_timesteps,\n",
    "    max_timestamp=ARGS.max_timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[None None]\n",
      " [None None]\n",
      " [None None]\n",
      " [None None]\n",
      " [None None]]\n",
      "Timestamp: 20231026_084723_925028\n",
      "0-th fold...\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 87)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 87)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 1)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "external_masking_1 (ExternalMas (None, None, 87)     0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 87)     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "external_masking_2 (ExternalMas (None, None, 1)      0           input_3[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "grud_1 (GRUD)                   (None, 64)           51694       external_masking_1[0][0]         \n",
      "                                                                 masking_1[0][0]                  \n",
      "                                                                 external_masking_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           grud_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 12)           780         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 52,474\n",
      "Trainable params: 52,474\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/jupyter-n.mekkes@gmail.com-f6d87/.conda/envs/temporal5/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      ".Epoch 1/50\n",
      "34/34 [==============================] - 3s 98ms/step - loss: 0.5849 - val_loss: 0.3764............\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.3250 - val_loss: 0.2610.............\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2632 - val_loss: 0.2341...........\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2415 - val_loss: 0.2191...........\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.2238 - val_loss: 0.2075.............\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2149 - val_loss: 0.1976...........\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2023 - val_loss: 0.1893...........\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1949 - val_loss: 0.1826...........\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1898 - val_loss: 0.1770.............\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1834 - val_loss: 0.1715...........\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1742 - val_loss: 0.1669..........\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1705 - val_loss: 0.1627.............\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1666 - val_loss: 0.1590...........\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1613 - val_loss: 0.1559...........\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 0.1583 - val_loss: 0.1523...........\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1533 - val_loss: 0.1494..........\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1490 - val_loss: 0.1469............\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1450 - val_loss: 0.1439.............\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1412 - val_loss: 0.1415............\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1425 - val_loss: 0.1394.............\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 0.1354 - val_loss: 0.1378.............\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1355 - val_loss: 0.1357............\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1316 - val_loss: 0.1341.............\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1306 - val_loss: 0.1334.............\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1282 - val_loss: 0.1325.............\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1264 - val_loss: 0.1310............\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 2s 56ms/step - loss: 0.1230 - val_loss: 0.1304...........\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 2s 60ms/step - loss: 0.1218 - val_loss: 0.1295.......\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 2s 57ms/step - loss: 0.1204 - val_loss: 0.1289.........\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 0.1180 - val_loss: 0.1280.........\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 0.1169 - val_loss: 0.1273...........\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1170 - val_loss: 0.1273.............\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1159 - val_loss: 0.1264............\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1150 - val_loss: 0.1260............\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1122 - val_loss: 0.1261...........\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1122 - val_loss: 0.1250 0.1114..............................\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1101 - val_loss: 0.1252.............\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1061 - val_loss: 0.1255.............\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1054 - val_loss: 0.125284...................................\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1034 - val_loss: 0.1247...........\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1046 - val_loss: 0.1239...........\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1051 - val_loss: 0.1238..........\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1015 - val_loss: 0.1233............\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1013 - val_loss: 0.1232............\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0996 - val_loss: 0.1237............\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0990 - val_loss: 0.1230...........\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0975 - val_loss: 0.1239...........\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0983 - val_loss: 0.1242............\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0977 - val_loss: 0.1240............\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0948 - val_loss: 0.1231............\n",
      ".....................................................................................AUC score of this fold: [0.982781046738423, 0.9320198715141551, 0.938287811110894]\n",
      "[[list([0.5851986362679448, 0.32500493169709027, 0.2632413377803553, 0.24152238952411034, 0.22390629628306155, 0.21488063350237535, 0.2023345353304791, 0.1948972836438683, 0.18980096110432626, 0.18337479069088045, 0.1741782530017101, 0.17052368007421934, 0.16652293234350055, 0.16127691364420052, 0.1582718267899631, 0.15328039182264283, 0.1490157733141388, 0.14502651359659532, 0.14118692907290345, 0.14248956273383637, 0.13528637129737967, 0.13547886825727495, 0.13154575730884932, 0.13060902366431817, 0.12815833045083955, 0.12636735405546526, 0.12299393492015027, 0.12172158342369353, 0.12031324715465033, 0.11800837077708096, 0.11691128783627769, 0.11700106790875861, 0.11597640204550373, 0.11504912262518323, 0.11214159168371618, 0.11213902733931884, 0.11012172070546264, 0.10610351283411953, 0.10537680639142709, 0.10346548769684787, 0.10462911594418971, 0.10512866111365911, 0.10142924181060457, 0.10130136665882986, 0.0995848059764005, 0.09892357981885555, 0.09748522907111307, 0.09833915497332427, 0.09766335793621632, 0.09477258763150835])\n",
      "  list([0.37639613721252146, 0.26102707168673944, 0.2341064504496959, 0.21912772583039425, 0.20751487344338748, 0.19755046530652443, 0.1892717358815736, 0.18260032358419828, 0.17699070274829865, 0.1714952295821016, 0.1669383583312535, 0.16267925006908607, 0.15904663330283614, 0.15591400367778968, 0.1523164490968483, 0.14941228489849448, 0.14686449777684818, 0.14394458807305077, 0.1415219909578397, 0.1393718665146696, 0.13777449463612468, 0.1357482779585854, 0.13410906806505846, 0.13344255089759827, 0.13251400397298085, 0.13095359825297614, 0.1303687319571142, 0.12947556252966927, 0.12892198842533387, 0.1280165576473784, 0.12726210905702073, 0.12733076060015852, 0.12638562950640095, 0.12604069800337375, 0.12610010465205704, 0.12504192799823718, 0.12516381249901998, 0.12546734841159693, 0.12518083933967253, 0.12470433469964655, 0.12392129539126191, 0.12384731474831619, 0.12330745004158652, 0.12318771320153336, 0.12372776758934252, 0.12300691082661981, 0.12387360410137072, 0.12423796235527122, 0.12404513606050396, 0.12306149674383975])]\n",
      " [None None]\n",
      " [None None]\n",
      " [None None]\n",
      " [None None]]\n",
      "1-th fold...\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 3s 83ms/step - loss: 0.4813 - val_loss: 0.3130..........\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.2905 - val_loss: 0.2427...........\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2485 - val_loss: 0.2226..........\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2326 - val_loss: 0.2082..........\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.2170 - val_loss: 0.1958...........\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.2031 - val_loss: 0.1859.........\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1944 - val_loss: 0.1779............\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1843 - val_loss: 0.1713............\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1783 - val_loss: 0.1659............\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1703 - val_loss: 0.1610...........\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1647 - val_loss: 0.1568...........\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1584 - val_loss: 0.1531...........\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1555 - val_loss: 0.1499............\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1509 - val_loss: 0.1471..........\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1452 - val_loss: 0.1445.............\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1429 - val_loss: 0.1423.............\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1390 - val_loss: 0.1400............\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1371 - val_loss: 0.1383.............\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1321 - val_loss: 0.1364...........\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1303 - val_loss: 0.1348............\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1290 - val_loss: 0.1336...........\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1265 - val_loss: 0.1322............\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1217 - val_loss: 0.1315...........\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1234 - val_loss: 0.1306............\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1192 - val_loss: 0.1297...........\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1191 - val_loss: 0.1290...........\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1179 - val_loss: 0.1280............\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1159 - val_loss: 0.1278............\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1122 - val_loss: 0.1270............\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 0.1135 - val_loss: 0.1268.........\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1101 - val_loss: 0.1262...........\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1086 - val_loss: 0.1247..........\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1063 - val_loss: 0.1246..........\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1086 - val_loss: 0.1243...........\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1065 - val_loss: 0.1240...........\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1065 - val_loss: 0.1240...........\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1036 - val_loss: 0.1239............\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1035 - val_loss: 0.1232............\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 0.1035 - val_loss: 0.1235..............\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1019 - val_loss: 0.1232............\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0999 - val_loss: 0.1228...........\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0982 - val_loss: 0.1230............\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0978 - val_loss: 0.1219...........\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0970 - val_loss: 0.1231............\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0954 - val_loss: 0.1228...........\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0954 - val_loss: 0.1221...........\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0946 - val_loss: 0.1232............\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0936 - val_loss: 0.1227...........\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0917 - val_loss: 0.1221............\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0943 - val_loss: 0.1221............\n",
      ".....................................................................................AUC score of this fold: [0.9824599274149133, 0.924444395270435, 0.9173141157279382]\n",
      "[[list([0.5851986362679448, 0.32500493169709027, 0.2632413377803553, 0.24152238952411034, 0.22390629628306155, 0.21488063350237535, 0.2023345353304791, 0.1948972836438683, 0.18980096110432626, 0.18337479069088045, 0.1741782530017101, 0.17052368007421934, 0.16652293234350055, 0.16127691364420052, 0.1582718267899631, 0.15328039182264283, 0.1490157733141388, 0.14502651359659532, 0.14118692907290345, 0.14248956273383637, 0.13528637129737967, 0.13547886825727495, 0.13154575730884932, 0.13060902366431817, 0.12815833045083955, 0.12636735405546526, 0.12299393492015027, 0.12172158342369353, 0.12031324715465033, 0.11800837077708096, 0.11691128783627769, 0.11700106790875861, 0.11597640204550373, 0.11504912262518323, 0.11214159168371618, 0.11213902733931884, 0.11012172070546264, 0.10610351283411953, 0.10537680639142709, 0.10346548769684787, 0.10462911594418971, 0.10512866111365911, 0.10142924181060457, 0.10130136665882986, 0.0995848059764005, 0.09892357981885555, 0.09748522907111307, 0.09833915497332427, 0.09766335793621632, 0.09477258763150835])\n",
      "  list([0.37639613721252146, 0.26102707168673944, 0.2341064504496959, 0.21912772583039425, 0.20751487344338748, 0.19755046530652443, 0.1892717358815736, 0.18260032358419828, 0.17699070274829865, 0.1714952295821016, 0.1669383583312535, 0.16267925006908607, 0.15904663330283614, 0.15591400367778968, 0.1523164490968483, 0.14941228489849448, 0.14686449777684818, 0.14394458807305077, 0.1415219909578397, 0.1393718665146696, 0.13777449463612468, 0.1357482779585854, 0.13410906806505846, 0.13344255089759827, 0.13251400397298085, 0.13095359825297614, 0.1303687319571142, 0.12947556252966927, 0.12892198842533387, 0.1280165576473784, 0.12726210905702073, 0.12733076060015852, 0.12638562950640095, 0.12604069800337375, 0.12610010465205704, 0.12504192799823718, 0.12516381249901998, 0.12546734841159693, 0.12518083933967253, 0.12470433469964655, 0.12392129539126191, 0.12384731474831619, 0.12330745004158652, 0.12318771320153336, 0.12372776758934252, 0.12300691082661981, 0.12387360410137072, 0.12423796235527122, 0.12404513606050396, 0.12306149674383975])]\n",
      " [list([0.4815725830707761, 0.29055172413532687, 0.24844171570589968, 0.23263579239283258, 0.21703199367167542, 0.20318616633037617, 0.19440957895629313, 0.1842080315448083, 0.17831628222162552, 0.17023573148975057, 0.16470529128064987, 0.1584205061855896, 0.15547772075699617, 0.15088105854847594, 0.14519075426387962, 0.14288176228438074, 0.138978013731059, 0.13712653723540227, 0.1320855855996657, 0.13026833547961866, 0.12899245784153157, 0.12656830927669233, 0.12170296224112011, 0.12339801987561073, 0.11921489395160043, 0.1190529282772519, 0.11792394189544804, 0.11584699171737632, 0.11224654791653706, 0.11358547420626845, 0.11006509453661974, 0.1085823706006477, 0.10629701440121368, 0.10858780908540687, 0.10636428978232389, 0.10652737943781454, 0.10365629814990537, 0.10347403740959712, 0.10345613050855984, 0.10189490774937976, 0.09991015620104296, 0.09822247893606102, 0.09781647684879285, 0.09699747474044287, 0.0954724765669136, 0.09536415589613151, 0.0945593138014414, 0.09349355889178991, 0.09172437474488333, 0.09428056232068639])\n",
      "  list([0.3129860106752722, 0.2426852664565513, 0.22261985082652688, 0.20819550802035885, 0.19580528098904626, 0.1859247658957434, 0.17791737574898736, 0.1713097656135401, 0.165881156180445, 0.16103139001032266, 0.15678145024328602, 0.15310489785605372, 0.14986943169522682, 0.14714733663514173, 0.14445445764789266, 0.14228169216635478, 0.14001411430084904, 0.13827534925542484, 0.13642495475421296, 0.13484192676636395, 0.13360684542036846, 0.13219498451902062, 0.13149297821916928, 0.13061461478307101, 0.12970483187812468, 0.12896792936390936, 0.12795941210583428, 0.1277906175147104, 0.12697169851532297, 0.12675089936559372, 0.12619659378713008, 0.12472902864053105, 0.12459858461638182, 0.12434829631563049, 0.12404211878117935, 0.12404621421302879, 0.12386787802145625, 0.12319919055338063, 0.123533779086329, 0.12321298672349414, 0.1228113140684465, 0.12303096541712956, 0.12185346948507741, 0.12312240075340587, 0.12282406048880097, 0.12211077112848587, 0.12322025964273274, 0.12268330123872388, 0.12213021961364957, 0.12213681358329499])]\n",
      " [None None]\n",
      " [None None]\n",
      " [None None]]\n",
      "2-th fold...\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 3s 84ms/step - loss: 0.4861 - val_loss: 0.3111..........\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2844 - val_loss: 0.2413............\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2463 - val_loss: 0.2220.............\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2292 - val_loss: 0.2091..........\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.2193 - val_loss: 0.1988............\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2055 - val_loss: 0.1901...........\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1987 - val_loss: 0.1824...........\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 0.1921 - val_loss: 0.1758..........\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1817 - val_loss: 0.1697.............\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 2s 54ms/step - loss: 0.1720 - val_loss: 0.1644..........\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1705 - val_loss: 0.1596.............\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1654 - val_loss: 0.1553............\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1580 - val_loss: 0.1516.............\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1534 - val_loss: 0.1482.............\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1505 - val_loss: 0.1454.............\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1478 - val_loss: 0.1428............\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1447 - val_loss: 0.1411............\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1396 - val_loss: 0.1387............\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1358 - val_loss: 0.1370.............\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1338 - val_loss: 0.1354............\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1304 - val_loss: 0.1342.............\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1298 - val_loss: 0.1331............\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1271 - val_loss: 0.1316.............\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 0.1255 - val_loss: 0.1301.............\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1223 - val_loss: 0.1297.............\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1233 - val_loss: 0.1292.............\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1182 - val_loss: 0.1278...........\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1187 - val_loss: 0.1273...........\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1168 - val_loss: 0.1264............\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1169 - val_loss: 0.1257.............\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1152 - val_loss: 0.1253............\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1127 - val_loss: 0.1249.............\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1096 - val_loss: 0.1247............\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1084 - val_loss: 0.1239.............\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1110 - val_loss: 0.1230...........\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1096 - val_loss: 0.1236...........\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1050 - val_loss: 0.1225............\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1028 - val_loss: 0.1224............\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1066 - val_loss: 0.1220............\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1033 - val_loss: 0.1226.............\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1022 - val_loss: 0.1224.............\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0986 - val_loss: 0.1221...........\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1002 - val_loss: 0.1221.............\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1019 - val_loss: 0.1214...........\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1027 - val_loss: 0.1225............\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0990 - val_loss: 0.1211............\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0947 - val_loss: 0.1210.............\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0971 - val_loss: 0.1211............\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0956 - val_loss: 0.1209..........\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0946 - val_loss: 0.1207...........\n",
      ".....................................................................................AUC score of this fold: [0.9868269579988116, 0.9323995757734961, 0.9486703873408447]\n",
      "[[list([0.5851986362679448, 0.32500493169709027, 0.2632413377803553, 0.24152238952411034, 0.22390629628306155, 0.21488063350237535, 0.2023345353304791, 0.1948972836438683, 0.18980096110432626, 0.18337479069088045, 0.1741782530017101, 0.17052368007421934, 0.16652293234350055, 0.16127691364420052, 0.1582718267899631, 0.15328039182264283, 0.1490157733141388, 0.14502651359659532, 0.14118692907290345, 0.14248956273383637, 0.13528637129737967, 0.13547886825727495, 0.13154575730884932, 0.13060902366431817, 0.12815833045083955, 0.12636735405546526, 0.12299393492015027, 0.12172158342369353, 0.12031324715465033, 0.11800837077708096, 0.11691128783627769, 0.11700106790875861, 0.11597640204550373, 0.11504912262518323, 0.11214159168371618, 0.11213902733931884, 0.11012172070546264, 0.10610351283411953, 0.10537680639142709, 0.10346548769684787, 0.10462911594418971, 0.10512866111365911, 0.10142924181060457, 0.10130136665882986, 0.0995848059764005, 0.09892357981885555, 0.09748522907111307, 0.09833915497332427, 0.09766335793621632, 0.09477258763150835])\n",
      "  list([0.37639613721252146, 0.26102707168673944, 0.2341064504496959, 0.21912772583039425, 0.20751487344338748, 0.19755046530652443, 0.1892717358815736, 0.18260032358419828, 0.17699070274829865, 0.1714952295821016, 0.1669383583312535, 0.16267925006908607, 0.15904663330283614, 0.15591400367778968, 0.1523164490968483, 0.14941228489849448, 0.14686449777684818, 0.14394458807305077, 0.1415219909578397, 0.1393718665146696, 0.13777449463612468, 0.1357482779585854, 0.13410906806505846, 0.13344255089759827, 0.13251400397298085, 0.13095359825297614, 0.1303687319571142, 0.12947556252966927, 0.12892198842533387, 0.1280165576473784, 0.12726210905702073, 0.12733076060015852, 0.12638562950640095, 0.12604069800337375, 0.12610010465205704, 0.12504192799823718, 0.12516381249901998, 0.12546734841159693, 0.12518083933967253, 0.12470433469964655, 0.12392129539126191, 0.12384731474831619, 0.12330745004158652, 0.12318771320153336, 0.12372776758934252, 0.12300691082661981, 0.12387360410137072, 0.12423796235527122, 0.12404513606050396, 0.12306149674383975])]\n",
      " [list([0.4815725830707761, 0.29055172413532687, 0.24844171570589968, 0.23263579239283258, 0.21703199367167542, 0.20318616633037617, 0.19440957895629313, 0.1842080315448083, 0.17831628222162552, 0.17023573148975057, 0.16470529128064987, 0.1584205061855896, 0.15547772075699617, 0.15088105854847594, 0.14519075426387962, 0.14288176228438074, 0.138978013731059, 0.13712653723540227, 0.1320855855996657, 0.13026833547961866, 0.12899245784153157, 0.12656830927669233, 0.12170296224112011, 0.12339801987561073, 0.11921489395160043, 0.1190529282772519, 0.11792394189544804, 0.11584699171737632, 0.11224654791653706, 0.11358547420626845, 0.11006509453661974, 0.1085823706006477, 0.10629701440121368, 0.10858780908540687, 0.10636428978232389, 0.10652737943781454, 0.10365629814990537, 0.10347403740959712, 0.10345613050855984, 0.10189490774937976, 0.09991015620104296, 0.09822247893606102, 0.09781647684879285, 0.09699747474044287, 0.0954724765669136, 0.09536415589613151, 0.0945593138014414, 0.09349355889178991, 0.09172437474488333, 0.09428056232068639])\n",
      "  list([0.3129860106752722, 0.2426852664565513, 0.22261985082652688, 0.20819550802035885, 0.19580528098904626, 0.1859247658957434, 0.17791737574898736, 0.1713097656135401, 0.165881156180445, 0.16103139001032266, 0.15678145024328602, 0.15310489785605372, 0.14986943169522682, 0.14714733663514173, 0.14445445764789266, 0.14228169216635478, 0.14001411430084904, 0.13827534925542484, 0.13642495475421296, 0.13484192676636395, 0.13360684542036846, 0.13219498451902062, 0.13149297821916928, 0.13061461478307101, 0.12970483187812468, 0.12896792936390936, 0.12795941210583428, 0.1277906175147104, 0.12697169851532297, 0.12675089936559372, 0.12619659378713008, 0.12472902864053105, 0.12459858461638182, 0.12434829631563049, 0.12404211878117935, 0.12404621421302879, 0.12386787802145625, 0.12319919055338063, 0.123533779086329, 0.12321298672349414, 0.1228113140684465, 0.12303096541712956, 0.12185346948507741, 0.12312240075340587, 0.12282406048880097, 0.12211077112848587, 0.12322025964273274, 0.12268330123872388, 0.12213021961364957, 0.12213681358329499])]\n",
      " [list([0.486366936954126, 0.28442038208740195, 0.24634464732740027, 0.22919691437735304, 0.21931783742448024, 0.20553275336327914, 0.19878989263243876, 0.1920931418572122, 0.18167565258058616, 0.17201984052170707, 0.1705495474198027, 0.16540499997512192, 0.15804808072293003, 0.15346432530748252, 0.15055928158386855, 0.1478423706734378, 0.14471163335865153, 0.13961197092827293, 0.13581951538831488, 0.13383832299467827, 0.1304822779684216, 0.1297964566073365, 0.12707574418565845, 0.12543842857823626, 0.12232112250604682, 0.12322696733321055, 0.11821708332877572, 0.11872338085104309, 0.11677158381069563, 0.11699387671264715, 0.11516845283916642, 0.11270924527232379, 0.10960008356287994, 0.10843510751511291, 0.11098870250840652, 0.10961377966469822, 0.10491997306792555, 0.10280274440625316, 0.10659282647663278, 0.10325581181115208, 0.10223648314317946, 0.09863509497095867, 0.10016899156636296, 0.10191383428226738, 0.10270271325418742, 0.09902909594291262, 0.09472034244521986, 0.09711041839357679, 0.09564964445805242, 0.09452793395925402])\n",
      "  list([0.31111249838086125, 0.24132901150218689, 0.2219950313067568, 0.20906601525143365, 0.19883916391193537, 0.19011566273415287, 0.1824404513309015, 0.17579249263797675, 0.16967446443454995, 0.1643963349128955, 0.15955641001298282, 0.1552791919003534, 0.151606527975251, 0.14824929421777883, 0.14538198832978202, 0.1427532569479547, 0.1410654629297678, 0.13874284988938115, 0.13703752873022912, 0.13539425098434996, 0.1342347051063295, 0.13308327583318258, 0.1316086894405481, 0.13013352000910933, 0.12970623406916035, 0.12916401753109463, 0.12784619958690516, 0.12733240730196074, 0.12641262258943273, 0.12574500224208304, 0.12525323033332825, 0.1249151907408435, 0.12472139414173464, 0.12393993954302857, 0.12301677226690956, 0.12360563610798746, 0.12252353625732232, 0.12236539756066232, 0.12204288703631301, 0.12261263556901922, 0.12237713885241451, 0.12206006395882664, 0.12211358003853434, 0.12138759744101467, 0.12253495861482884, 0.12114146788146614, 0.12096089412823566, 0.12114531715601189, 0.12092824782455824, 0.12073220007985995])]\n",
      " [None None]\n",
      " [None None]]\n",
      "3-th fold...\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 3s 84ms/step - loss: 0.4855 - val_loss: 0.3190..........\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.2892 - val_loss: 0.2433............\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.2448 - val_loss: 0.2221.............\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.2285 - val_loss: 0.2074..............\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2147 - val_loss: 0.1964............\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2012 - val_loss: 0.1872...........\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1927 - val_loss: 0.1799.............\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1833 - val_loss: 0.1735............\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1784 - val_loss: 0.1675............\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1698 - val_loss: 0.1623............\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1652 - val_loss: 0.1573...........\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1595 - val_loss: 0.1530.............\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1531 - val_loss: 0.1492...........\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1526 - val_loss: 0.1456............\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1485 - val_loss: 0.1428............\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1454 - val_loss: 0.1400...........\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1417 - val_loss: 0.1373..............\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1365 - val_loss: 0.1350.............\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1326 - val_loss: 0.1328...........\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1323 - val_loss: 0.1307.............\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1329 - val_loss: 0.1295.............\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1272 - val_loss: 0.1277...........\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1237 - val_loss: 0.1266.............\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 2s 55ms/step - loss: 0.1242 - val_loss: 0.1248............\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1220 - val_loss: 0.1238.............\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1200 - val_loss: 0.1228............\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1188 - val_loss: 0.1215...........\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1165 - val_loss: 0.1211.............\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1175 - val_loss: 0.1205..........\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1120 - val_loss: 0.1201...........\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1130 - val_loss: 0.1192............\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1107 - val_loss: 0.1185...........\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1104 - val_loss: 0.1180.............\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1084 - val_loss: 0.1167............\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1064 - val_loss: 0.1168............\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1017 - val_loss: 0.1163............\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1045 - val_loss: 0.1162.............\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1032 - val_loss: 0.1153.............\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1001 - val_loss: 0.1159............\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1022 - val_loss: 0.1173.............\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1026 - val_loss: 0.1159...........\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1027 - val_loss: 0.1156............\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0965 - val_loss: 0.1152...........\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0975 - val_loss: 0.1146.............\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0953 - val_loss: 0.1143...........\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.0977 - val_loss: 0.1142............\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0946 - val_loss: 0.1145...........\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0967 - val_loss: 0.1147............\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0946 - val_loss: 0.1144............\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0925 - val_loss: 0.1152............\n",
      ".....................................................................................AUC score of this fold: [0.9847396439740769, 0.926752596869096, 0.9281234963804955]\n",
      "[[list([0.5851986362679448, 0.32500493169709027, 0.2632413377803553, 0.24152238952411034, 0.22390629628306155, 0.21488063350237535, 0.2023345353304791, 0.1948972836438683, 0.18980096110432626, 0.18337479069088045, 0.1741782530017101, 0.17052368007421934, 0.16652293234350055, 0.16127691364420052, 0.1582718267899631, 0.15328039182264283, 0.1490157733141388, 0.14502651359659532, 0.14118692907290345, 0.14248956273383637, 0.13528637129737967, 0.13547886825727495, 0.13154575730884932, 0.13060902366431817, 0.12815833045083955, 0.12636735405546526, 0.12299393492015027, 0.12172158342369353, 0.12031324715465033, 0.11800837077708096, 0.11691128783627769, 0.11700106790875861, 0.11597640204550373, 0.11504912262518323, 0.11214159168371618, 0.11213902733931884, 0.11012172070546264, 0.10610351283411953, 0.10537680639142709, 0.10346548769684787, 0.10462911594418971, 0.10512866111365911, 0.10142924181060457, 0.10130136665882986, 0.0995848059764005, 0.09892357981885555, 0.09748522907111307, 0.09833915497332427, 0.09766335793621632, 0.09477258763150835])\n",
      "  list([0.37639613721252146, 0.26102707168673944, 0.2341064504496959, 0.21912772583039425, 0.20751487344338748, 0.19755046530652443, 0.1892717358815736, 0.18260032358419828, 0.17699070274829865, 0.1714952295821016, 0.1669383583312535, 0.16267925006908607, 0.15904663330283614, 0.15591400367778968, 0.1523164490968483, 0.14941228489849448, 0.14686449777684818, 0.14394458807305077, 0.1415219909578397, 0.1393718665146696, 0.13777449463612468, 0.1357482779585854, 0.13410906806505846, 0.13344255089759827, 0.13251400397298085, 0.13095359825297614, 0.1303687319571142, 0.12947556252966927, 0.12892198842533387, 0.1280165576473784, 0.12726210905702073, 0.12733076060015852, 0.12638562950640095, 0.12604069800337375, 0.12610010465205704, 0.12504192799823718, 0.12516381249901998, 0.12546734841159693, 0.12518083933967253, 0.12470433469964655, 0.12392129539126191, 0.12384731474831619, 0.12330745004158652, 0.12318771320153336, 0.12372776758934252, 0.12300691082661981, 0.12387360410137072, 0.12423796235527122, 0.12404513606050396, 0.12306149674383975])]\n",
      " [list([0.4815725830707761, 0.29055172413532687, 0.24844171570589968, 0.23263579239283258, 0.21703199367167542, 0.20318616633037617, 0.19440957895629313, 0.1842080315448083, 0.17831628222162552, 0.17023573148975057, 0.16470529128064987, 0.1584205061855896, 0.15547772075699617, 0.15088105854847594, 0.14519075426387962, 0.14288176228438074, 0.138978013731059, 0.13712653723540227, 0.1320855855996657, 0.13026833547961866, 0.12899245784153157, 0.12656830927669233, 0.12170296224112011, 0.12339801987561073, 0.11921489395160043, 0.1190529282772519, 0.11792394189544804, 0.11584699171737632, 0.11224654791653706, 0.11358547420626845, 0.11006509453661974, 0.1085823706006477, 0.10629701440121368, 0.10858780908540687, 0.10636428978232389, 0.10652737943781454, 0.10365629814990537, 0.10347403740959712, 0.10345613050855984, 0.10189490774937976, 0.09991015620104296, 0.09822247893606102, 0.09781647684879285, 0.09699747474044287, 0.0954724765669136, 0.09536415589613151, 0.0945593138014414, 0.09349355889178991, 0.09172437474488333, 0.09428056232068639])\n",
      "  list([0.3129860106752722, 0.2426852664565513, 0.22261985082652688, 0.20819550802035885, 0.19580528098904626, 0.1859247658957434, 0.17791737574898736, 0.1713097656135401, 0.165881156180445, 0.16103139001032266, 0.15678145024328602, 0.15310489785605372, 0.14986943169522682, 0.14714733663514173, 0.14445445764789266, 0.14228169216635478, 0.14001411430084904, 0.13827534925542484, 0.13642495475421296, 0.13484192676636395, 0.13360684542036846, 0.13219498451902062, 0.13149297821916928, 0.13061461478307101, 0.12970483187812468, 0.12896792936390936, 0.12795941210583428, 0.1277906175147104, 0.12697169851532297, 0.12675089936559372, 0.12619659378713008, 0.12472902864053105, 0.12459858461638182, 0.12434829631563049, 0.12404211878117935, 0.12404621421302879, 0.12386787802145625, 0.12319919055338063, 0.123533779086329, 0.12321298672349414, 0.1228113140684465, 0.12303096541712956, 0.12185346948507741, 0.12312240075340587, 0.12282406048880097, 0.12211077112848587, 0.12322025964273274, 0.12268330123872388, 0.12213021961364957, 0.12213681358329499])]\n",
      " [list([0.486366936954126, 0.28442038208740195, 0.24634464732740027, 0.22919691437735304, 0.21931783742448024, 0.20553275336327914, 0.19878989263243876, 0.1920931418572122, 0.18167565258058616, 0.17201984052170707, 0.1705495474198027, 0.16540499997512192, 0.15804808072293003, 0.15346432530748252, 0.15055928158386855, 0.1478423706734378, 0.14471163335865153, 0.13961197092827293, 0.13581951538831488, 0.13383832299467827, 0.1304822779684216, 0.1297964566073365, 0.12707574418565845, 0.12543842857823626, 0.12232112250604682, 0.12322696733321055, 0.11821708332877572, 0.11872338085104309, 0.11677158381069563, 0.11699387671264715, 0.11516845283916642, 0.11270924527232379, 0.10960008356287994, 0.10843510751511291, 0.11098870250840652, 0.10961377966469822, 0.10491997306792555, 0.10280274440625316, 0.10659282647663278, 0.10325581181115208, 0.10223648314317946, 0.09863509497095867, 0.10016899156636296, 0.10191383428226738, 0.10270271325418742, 0.09902909594291262, 0.09472034244521986, 0.09711041839357679, 0.09564964445805242, 0.09452793395925402])\n",
      "  list([0.31111249838086125, 0.24132901150218689, 0.2219950313067568, 0.20906601525143365, 0.19883916391193537, 0.19011566273415287, 0.1824404513309015, 0.17579249263797675, 0.16967446443454995, 0.1643963349128955, 0.15955641001298282, 0.1552791919003534, 0.151606527975251, 0.14824929421777883, 0.14538198832978202, 0.1427532569479547, 0.1410654629297678, 0.13874284988938115, 0.13703752873022912, 0.13539425098434996, 0.1342347051063295, 0.13308327583318258, 0.1316086894405481, 0.13013352000910933, 0.12970623406916035, 0.12916401753109463, 0.12784619958690516, 0.12733240730196074, 0.12641262258943273, 0.12574500224208304, 0.12525323033332825, 0.1249151907408435, 0.12472139414173464, 0.12393993954302857, 0.12301677226690956, 0.12360563610798746, 0.12252353625732232, 0.12236539756066232, 0.12204288703631301, 0.12261263556901922, 0.12237713885241451, 0.12206006395882664, 0.12211358003853434, 0.12138759744101467, 0.12253495861482884, 0.12114146788146614, 0.12096089412823566, 0.12114531715601189, 0.12092824782455824, 0.12073220007985995])]\n",
      " [list([0.48571409430512624, 0.28919139784343995, 0.24484171824011794, 0.22860567304640186, 0.21471974788779052, 0.20117677309126705, 0.1927119418292634, 0.18336199218506752, 0.17849997726044384, 0.16985301807879524, 0.16519436012425256, 0.1594676025774378, 0.15312972258687238, 0.15267231928709463, 0.14856714803320709, 0.14537115812191867, 0.14171648319145153, 0.13654430646922708, 0.132545191664283, 0.13230786361417718, 0.1328838703876042, 0.12714299477266344, 0.12373016225808672, 0.12417638979673824, 0.12201318778715081, 0.11999313438630235, 0.11882209666690774, 0.11654126361478978, 0.11743722482829683, 0.1120035929080531, 0.11302599423388311, 0.11065819928110414, 0.11039671115234193, 0.10837075803051557, 0.10634121503235007, 0.10170548421572585, 0.1044570132403962, 0.103208654287441, 0.10016978270002053, 0.10218148227115582, 0.10255791710665652, 0.10269807582347212, 0.09646420724154836, 0.09745820695414288, 0.09520831920933767, 0.09777354774224824, 0.09465678528801952, 0.09676631383639014, 0.09458624671726157, 0.0925172779496423])\n",
      "  list([0.31895075796058825, 0.24326764763389502, 0.22214538995073646, 0.20740681573830916, 0.19643126683340548, 0.18721352180064713, 0.17989963852898191, 0.17350198130910569, 0.16750153940356238, 0.16226967806974169, 0.15729188984928869, 0.15298021786450022, 0.14919149945618698, 0.1456383737575942, 0.142793459828058, 0.14000761010864163, 0.13733074632962106, 0.1349968438484392, 0.13280130910445315, 0.13074036206000417, 0.12954893004334433, 0.12768935786755703, 0.12660166394809333, 0.12478990227148677, 0.12382895437722706, 0.12277477176973174, 0.12153398043872243, 0.12112110692657818, 0.12054545193581291, 0.12008730968388405, 0.11918986971536394, 0.11850221290443484, 0.11799971528804104, 0.11669170547750114, 0.1168261829825396, 0.11630199866070932, 0.11621789901790039, 0.11526037725872097, 0.11593332802310832, 0.11725735565575447, 0.1158870644661603, 0.11560450568383569, 0.11524270386409365, 0.11463274944635386, 0.1143170858409194, 0.11416604734175113, 0.11454241037451102, 0.114689191525483, 0.11441876204138961, 0.11515664200180144])]\n",
      " [None None]]\n",
      "4-th fold...\n",
      "Epoch 1/50\n",
      "34/34 [==============================] - 3s 84ms/step - loss: 0.4816 - val_loss: 0.3068..........\n",
      "Epoch 2/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.2841 - val_loss: 0.2409.............\n",
      "Epoch 3/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2476 - val_loss: 0.2241.............\n",
      "Epoch 4/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.2311 - val_loss: 0.2107............\n",
      "Epoch 5/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.2181 - val_loss: 0.1995.............\n",
      "Epoch 6/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.2058 - val_loss: 0.1899...........\n",
      "Epoch 7/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1972 - val_loss: 0.1816............\n",
      "Epoch 8/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1894 - val_loss: 0.1741...........\n",
      "Epoch 9/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1803 - val_loss: 0.1680...........\n",
      "Epoch 10/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1727 - val_loss: 0.1623...........\n",
      "Epoch 11/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1673 - val_loss: 0.1572...........\n",
      "Epoch 12/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1613 - val_loss: 0.1533............\n",
      "Epoch 13/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1582 - val_loss: 0.1492............\n",
      "Epoch 14/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1551 - val_loss: 0.1458.............\n",
      "Epoch 15/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1488 - val_loss: 0.1431............\n",
      "Epoch 16/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1484 - val_loss: 0.1396............\n",
      "Epoch 17/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1412 - val_loss: 0.1372............\n",
      "Epoch 18/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1407 - val_loss: 0.1349...........\n",
      "Epoch 19/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1361 - val_loss: 0.1322............\n",
      "Epoch 20/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1348 - val_loss: 0.1304............\n",
      "Epoch 21/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1312 - val_loss: 0.1289...........\n",
      "Epoch 22/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1308 - val_loss: 0.1266............\n",
      "Epoch 23/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1286 - val_loss: 0.1253............\n",
      "Epoch 24/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1231 - val_loss: 0.1236..........\n",
      "Epoch 25/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1216 - val_loss: 0.1228............\n",
      "Epoch 26/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1195 - val_loss: 0.1214............\n",
      "Epoch 27/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1170 - val_loss: 0.1211...........\n",
      "Epoch 28/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1152 - val_loss: 0.1205............\n",
      "Epoch 29/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.1165 - val_loss: 0.1197............\n",
      "Epoch 30/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1138 - val_loss: 0.1181.............\n",
      "Epoch 31/50\n",
      "34/34 [==============================] - 2s 50ms/step - loss: 0.1117 - val_loss: 0.1185...........\n",
      "Epoch 32/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1098 - val_loss: 0.1181...........\n",
      "Epoch 33/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1122 - val_loss: 0.1171.............\n",
      "Epoch 34/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1099 - val_loss: 0.1168.............\n",
      "Epoch 35/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1060 - val_loss: 0.1153............\n",
      "Epoch 36/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1066 - val_loss: 0.1166..........\n",
      "Epoch 37/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1049 - val_loss: 0.1161............\n",
      "Epoch 38/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1055 - val_loss: 0.1153...........\n",
      "Epoch 39/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1031 - val_loss: 0.1144.............\n",
      "Epoch 40/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.1023 - val_loss: 0.1143............\n",
      "Epoch 41/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1005 - val_loss: 0.1136.............\n",
      "Epoch 42/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.1039 - val_loss: 0.1130...........\n",
      "Epoch 43/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0982 - val_loss: 0.1135............\n",
      "Epoch 44/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0979 - val_loss: 0.1122...........\n",
      "Epoch 45/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0950 - val_loss: 0.1126............\n",
      "Epoch 46/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0984 - val_loss: 0.1122............\n",
      "Epoch 47/50\n",
      "34/34 [==============================] - 2s 53ms/step - loss: 0.0961 - val_loss: 0.1130...........\n",
      "Epoch 48/50\n",
      "34/34 [==============================] - 2s 52ms/step - loss: 0.0951 - val_loss: 0.1118.............\n",
      "Epoch 49/50\n",
      "34/34 [==============================] - 2s 49ms/step - loss: 0.0938 - val_loss: 0.1115............\n",
      "Epoch 50/50\n",
      "34/34 [==============================] - 2s 51ms/step - loss: 0.0942 - val_loss: 0.1126971..................................\n",
      ".....................................................................................AUC score of this fold: [0.9841638898248329, 0.946198590726559, 0.9233108033477057]\n",
      "[[list([0.5851986362679448, 0.32500493169709027, 0.2632413377803553, 0.24152238952411034, 0.22390629628306155, 0.21488063350237535, 0.2023345353304791, 0.1948972836438683, 0.18980096110432626, 0.18337479069088045, 0.1741782530017101, 0.17052368007421934, 0.16652293234350055, 0.16127691364420052, 0.1582718267899631, 0.15328039182264283, 0.1490157733141388, 0.14502651359659532, 0.14118692907290345, 0.14248956273383637, 0.13528637129737967, 0.13547886825727495, 0.13154575730884932, 0.13060902366431817, 0.12815833045083955, 0.12636735405546526, 0.12299393492015027, 0.12172158342369353, 0.12031324715465033, 0.11800837077708096, 0.11691128783627769, 0.11700106790875861, 0.11597640204550373, 0.11504912262518323, 0.11214159168371618, 0.11213902733931884, 0.11012172070546264, 0.10610351283411953, 0.10537680639142709, 0.10346548769684787, 0.10462911594418971, 0.10512866111365911, 0.10142924181060457, 0.10130136665882986, 0.0995848059764005, 0.09892357981885555, 0.09748522907111307, 0.09833915497332427, 0.09766335793621632, 0.09477258763150835])\n",
      "  list([0.37639613721252146, 0.26102707168673944, 0.2341064504496959, 0.21912772583039425, 0.20751487344338748, 0.19755046530652443, 0.1892717358815736, 0.18260032358419828, 0.17699070274829865, 0.1714952295821016, 0.1669383583312535, 0.16267925006908607, 0.15904663330283614, 0.15591400367778968, 0.1523164490968483, 0.14941228489849448, 0.14686449777684818, 0.14394458807305077, 0.1415219909578397, 0.1393718665146696, 0.13777449463612468, 0.1357482779585854, 0.13410906806505846, 0.13344255089759827, 0.13251400397298085, 0.13095359825297614, 0.1303687319571142, 0.12947556252966927, 0.12892198842533387, 0.1280165576473784, 0.12726210905702073, 0.12733076060015852, 0.12638562950640095, 0.12604069800337375, 0.12610010465205704, 0.12504192799823718, 0.12516381249901998, 0.12546734841159693, 0.12518083933967253, 0.12470433469964655, 0.12392129539126191, 0.12384731474831619, 0.12330745004158652, 0.12318771320153336, 0.12372776758934252, 0.12300691082661981, 0.12387360410137072, 0.12423796235527122, 0.12404513606050396, 0.12306149674383975])]\n",
      " [list([0.4815725830707761, 0.29055172413532687, 0.24844171570589968, 0.23263579239283258, 0.21703199367167542, 0.20318616633037617, 0.19440957895629313, 0.1842080315448083, 0.17831628222162552, 0.17023573148975057, 0.16470529128064987, 0.1584205061855896, 0.15547772075699617, 0.15088105854847594, 0.14519075426387962, 0.14288176228438074, 0.138978013731059, 0.13712653723540227, 0.1320855855996657, 0.13026833547961866, 0.12899245784153157, 0.12656830927669233, 0.12170296224112011, 0.12339801987561073, 0.11921489395160043, 0.1190529282772519, 0.11792394189544804, 0.11584699171737632, 0.11224654791653706, 0.11358547420626845, 0.11006509453661974, 0.1085823706006477, 0.10629701440121368, 0.10858780908540687, 0.10636428978232389, 0.10652737943781454, 0.10365629814990537, 0.10347403740959712, 0.10345613050855984, 0.10189490774937976, 0.09991015620104296, 0.09822247893606102, 0.09781647684879285, 0.09699747474044287, 0.0954724765669136, 0.09536415589613151, 0.0945593138014414, 0.09349355889178991, 0.09172437474488333, 0.09428056232068639])\n",
      "  list([0.3129860106752722, 0.2426852664565513, 0.22261985082652688, 0.20819550802035885, 0.19580528098904626, 0.1859247658957434, 0.17791737574898736, 0.1713097656135401, 0.165881156180445, 0.16103139001032266, 0.15678145024328602, 0.15310489785605372, 0.14986943169522682, 0.14714733663514173, 0.14445445764789266, 0.14228169216635478, 0.14001411430084904, 0.13827534925542484, 0.13642495475421296, 0.13484192676636395, 0.13360684542036846, 0.13219498451902062, 0.13149297821916928, 0.13061461478307101, 0.12970483187812468, 0.12896792936390936, 0.12795941210583428, 0.1277906175147104, 0.12697169851532297, 0.12675089936559372, 0.12619659378713008, 0.12472902864053105, 0.12459858461638182, 0.12434829631563049, 0.12404211878117935, 0.12404621421302879, 0.12386787802145625, 0.12319919055338063, 0.123533779086329, 0.12321298672349414, 0.1228113140684465, 0.12303096541712956, 0.12185346948507741, 0.12312240075340587, 0.12282406048880097, 0.12211077112848587, 0.12322025964273274, 0.12268330123872388, 0.12213021961364957, 0.12213681358329499])]\n",
      " [list([0.486366936954126, 0.28442038208740195, 0.24634464732740027, 0.22919691437735304, 0.21931783742448024, 0.20553275336327914, 0.19878989263243876, 0.1920931418572122, 0.18167565258058616, 0.17201984052170707, 0.1705495474198027, 0.16540499997512192, 0.15804808072293003, 0.15346432530748252, 0.15055928158386855, 0.1478423706734378, 0.14471163335865153, 0.13961197092827293, 0.13581951538831488, 0.13383832299467827, 0.1304822779684216, 0.1297964566073365, 0.12707574418565845, 0.12543842857823626, 0.12232112250604682, 0.12322696733321055, 0.11821708332877572, 0.11872338085104309, 0.11677158381069563, 0.11699387671264715, 0.11516845283916642, 0.11270924527232379, 0.10960008356287994, 0.10843510751511291, 0.11098870250840652, 0.10961377966469822, 0.10491997306792555, 0.10280274440625316, 0.10659282647663278, 0.10325581181115208, 0.10223648314317946, 0.09863509497095867, 0.10016899156636296, 0.10191383428226738, 0.10270271325418742, 0.09902909594291262, 0.09472034244521986, 0.09711041839357679, 0.09564964445805242, 0.09452793395925402])\n",
      "  list([0.31111249838086125, 0.24132901150218689, 0.2219950313067568, 0.20906601525143365, 0.19883916391193537, 0.19011566273415287, 0.1824404513309015, 0.17579249263797675, 0.16967446443454995, 0.1643963349128955, 0.15955641001298282, 0.1552791919003534, 0.151606527975251, 0.14824929421777883, 0.14538198832978202, 0.1427532569479547, 0.1410654629297678, 0.13874284988938115, 0.13703752873022912, 0.13539425098434996, 0.1342347051063295, 0.13308327583318258, 0.1316086894405481, 0.13013352000910933, 0.12970623406916035, 0.12916401753109463, 0.12784619958690516, 0.12733240730196074, 0.12641262258943273, 0.12574500224208304, 0.12525323033332825, 0.1249151907408435, 0.12472139414173464, 0.12393993954302857, 0.12301677226690956, 0.12360563610798746, 0.12252353625732232, 0.12236539756066232, 0.12204288703631301, 0.12261263556901922, 0.12237713885241451, 0.12206006395882664, 0.12211358003853434, 0.12138759744101467, 0.12253495861482884, 0.12114146788146614, 0.12096089412823566, 0.12114531715601189, 0.12092824782455824, 0.12073220007985995])]\n",
      " [list([0.48571409430512624, 0.28919139784343995, 0.24484171824011794, 0.22860567304640186, 0.21471974788779052, 0.20117677309126705, 0.1927119418292634, 0.18336199218506752, 0.17849997726044384, 0.16985301807879524, 0.16519436012425256, 0.1594676025774378, 0.15312972258687238, 0.15267231928709463, 0.14856714803320709, 0.14537115812191867, 0.14171648319145153, 0.13654430646922708, 0.132545191664283, 0.13230786361417718, 0.1328838703876042, 0.12714299477266344, 0.12373016225808672, 0.12417638979673824, 0.12201318778715081, 0.11999313438630235, 0.11882209666690774, 0.11654126361478978, 0.11743722482829683, 0.1120035929080531, 0.11302599423388311, 0.11065819928110414, 0.11039671115234193, 0.10837075803051557, 0.10634121503235007, 0.10170548421572585, 0.1044570132403962, 0.103208654287441, 0.10016978270002053, 0.10218148227115582, 0.10255791710665652, 0.10269807582347212, 0.09646420724154836, 0.09745820695414288, 0.09520831920933767, 0.09777354774224824, 0.09465678528801952, 0.09676631383639014, 0.09458624671726157, 0.0925172779496423])\n",
      "  list([0.31895075796058825, 0.24326764763389502, 0.22214538995073646, 0.20740681573830916, 0.19643126683340548, 0.18721352180064713, 0.17989963852898191, 0.17350198130910569, 0.16750153940356238, 0.16226967806974169, 0.15729188984928869, 0.15298021786450022, 0.14919149945618698, 0.1456383737575942, 0.142793459828058, 0.14000761010864163, 0.13733074632962106, 0.1349968438484392, 0.13280130910445315, 0.13074036206000417, 0.12954893004334433, 0.12768935786755703, 0.12660166394809333, 0.12478990227148677, 0.12382895437722706, 0.12277477176973174, 0.12153398043872243, 0.12112110692657818, 0.12054545193581291, 0.12008730968388405, 0.11918986971536394, 0.11850221290443484, 0.11799971528804104, 0.11669170547750114, 0.1168261829825396, 0.11630199866070932, 0.11621789901790039, 0.11526037725872097, 0.11593332802310832, 0.11725735565575447, 0.1158870644661603, 0.11560450568383569, 0.11524270386409365, 0.11463274944635386, 0.1143170858409194, 0.11416604734175113, 0.11454241037451102, 0.114689191525483, 0.11441876204138961, 0.11515664200180144])]\n",
      " [list([0.4818867690326101, 0.28414112077014003, 0.2476367505874423, 0.23117938143772315, 0.2181625098374228, 0.20586066513320578, 0.19722593201128819, 0.18943896398469667, 0.18032760503761897, 0.17268978007151498, 0.16734122458852238, 0.16126415838743002, 0.15828100129495448, 0.15509543659827546, 0.14878479966252328, 0.14837473894351094, 0.14117513026212022, 0.14075460894167094, 0.1360512275118415, 0.13474821734274728, 0.1311972077874189, 0.1308152946758446, 0.1285529349061007, 0.12309877675980274, 0.12156624235830255, 0.11952819137338336, 0.1169767281517359, 0.11523159140546019, 0.11654863783722644, 0.11378635191675927, 0.11174752386235401, 0.10984971700836502, 0.11219691549655922, 0.1098947926367844, 0.10597051523667014, 0.10661549594520864, 0.10483910935085343, 0.10548811282406854, 0.1030908763326334, 0.10225635816379147, 0.10055546970218146, 0.10389950002919245, 0.0981740909002543, 0.09786432794883546, 0.09506826803884015, 0.09835628649257604, 0.09610074625434577, 0.09516941451400683, 0.09375971934413382, 0.0942595944522055])\n",
      "  list([0.30677005088790343, 0.24091682644838786, 0.22413844281797252, 0.2107159842773037, 0.19954614580006888, 0.18992654839273315, 0.1815778091138239, 0.17411435029124686, 0.16796580195756247, 0.16234002077118467, 0.1572276470740197, 0.15326946422210713, 0.14922486434983945, 0.14577627058516548, 0.14306309415819896, 0.1396103879611795, 0.1371717494989627, 0.134857219597582, 0.13221594242595178, 0.1304232164476458, 0.1288606219034827, 0.12662866793943373, 0.12525787513229727, 0.12363301283417485, 0.12276328211001929, 0.12143754469261643, 0.12109265562908425, 0.12048990123015083, 0.11972343118974517, 0.11814097726707301, 0.1185334291905988, 0.11812432093844229, 0.1170997764112541, 0.11679488651001652, 0.11531908509645673, 0.11660936483032795, 0.11607176353588947, 0.1152891592837829, 0.11441865653952182, 0.11427799505258791, 0.11362773945647708, 0.11304167818627964, 0.11353006316811999, 0.11217157427447935, 0.11263155566723966, 0.11222626384121279, 0.11298282164565766, 0.11175056378156441, 0.11153278997919178, 0.11255659640493973])]]\n",
      "Finished! ====================\n",
      "Mean AUC score: [0.98419429 0.93236301 0.93114132]; Std AUC score: [0.00156426 0.00755941 0.01113514]\n"
     ]
    }
   ],
   "source": [
    "# k-fold cross-validation\n",
    "pred_y_list_all = []\n",
    "auc_score_list_all = []\n",
    "true_y_list_all = [] # add by nienke\n",
    "losses = np.empty(shape=(5, 2), dtype=object)\n",
    "print(losses)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "print('Timestamp: {}'.format(timestamp))\n",
    "\n",
    "for i_fold in range(dataset.folds):\n",
    "    print('{}-th fold...'.format(i_fold))\n",
    "\n",
    "    # Load or train the model.\n",
    "    if ARGS.pretrained_model_file is not None:\n",
    "        model = load_grud_model(os.path.join(ARGS.working_path, \n",
    "                                             ARGS.pretrained_model_file.format(i_fold=i_fold)))\n",
    "    else:\n",
    "        model = create_grud_model(input_dim=dataset.input_dim,\n",
    "                                  output_dim=dataset.output_dim,\n",
    "                                  output_activation=dataset.output_activation,\n",
    "                                  recurrent_dim=ARGS.recurrent_dim,\n",
    "                                  hidden_dim=ARGS.hidden_dim,\n",
    "                                  predefined_model=ARGS.model,\n",
    "                                  use_bidirectional_rnn=ARGS.use_bidirectional_rnn\n",
    "                                 )\n",
    "        if i_fold == 0:\n",
    "            model.summary()\n",
    "        model.compile(optimizer='adam', loss=dataset.loss_function)\n",
    "        history = model.fit_generator(\n",
    "            generator=dataset.training_generator(i_fold, batch_size=ARGS.batch_size),\n",
    "            steps_per_epoch=dataset.training_steps(i_fold, batch_size=ARGS.batch_size),\n",
    "            epochs=ARGS.epochs,\n",
    "            verbose=1,\n",
    "            validation_data=dataset.validation_generator(i_fold, batch_size=ARGS.batch_size),\n",
    "            validation_steps=dataset.validation_steps(i_fold, batch_size=ARGS.batch_size),\n",
    "            callbacks=[\n",
    "                EarlyStopping(patience=ARGS.early_stopping_patience),\n",
    "                ModelCheckpointwithBestWeights(\n",
    "                    file_dir=os.path.join(ARGS.working_path, 'model', save_name + '_' +timestamp + '_' + str(i_fold))\n",
    "                ),\n",
    "                TensorBoard(\n",
    "                    log_dir=os.path.join(ARGS.working_path, 'tb_logs', save_name + '_' +timestamp + '_' + str(i_fold))\n",
    "                )\n",
    "            ]\n",
    "            )\n",
    "#         print(history.history.keys())\n",
    "#         print(history.history['loss'])\n",
    "#         print(history.history['val_loss'])\n",
    "        model.save(os.path.join(ARGS.working_path, 'model', \n",
    "                                save_name + '_' + timestamp + '_' + str(i_fold), 'model.h5'))\n",
    "\n",
    "    # Evaluate the model\n",
    "    true_y_list = [\n",
    "        dataset.training_y(i_fold), dataset.validation_y(i_fold), dataset.testing_y(i_fold)\n",
    "    ]\n",
    "    pred_y_list = [\n",
    "        model.predict_generator(dataset.training_generator_x(i_fold, batch_size=ARGS.batch_size),\n",
    "                                steps=dataset.training_steps(i_fold, batch_size=ARGS.batch_size)),\n",
    "        model.predict_generator(dataset.validation_generator_x(i_fold, batch_size=ARGS.batch_size),\n",
    "                                steps=dataset.validation_steps(i_fold, batch_size=ARGS.batch_size)),\n",
    "        model.predict_generator(dataset.testing_generator_x(i_fold, batch_size=ARGS.batch_size),\n",
    "                                steps=dataset.testing_steps(i_fold, batch_size=ARGS.batch_size)),\n",
    "    ]\n",
    "    auc_score_list = [roc_auc_score(ty, py) for ty, py in zip(true_y_list, pred_y_list)] # [3, n_task]\n",
    "    print('AUC score of this fold: {}'.format(auc_score_list))\n",
    "    pred_y_list_all.append(pred_y_list)\n",
    "    auc_score_list_all.append(auc_score_list)\n",
    "    true_y_list_all.append(true_y_list) # add by nienke\n",
    "#     print(history.history.keys())\n",
    "#     print(history.history['loss'])\n",
    "#     print(history.history['val_loss'])\n",
    "    losses[i_fold][0] = history.history['loss']\n",
    "    losses[i_fold][1] = history.history['val_loss']\n",
    "    print(losses)\n",
    "\n",
    "print('Finished!', '='*20)\n",
    "auc_score_list_all = np.stack(auc_score_list_all, axis=0)\n",
    "print('Mean AUC score: {}; Std AUC score: {}'.format(\n",
    "    np.mean(auc_score_list_all, axis=0),\n",
    "    np.std(auc_score_list_all, axis=0)))\n",
    "\n",
    "result_path = os.path.join(ARGS.working_path, 'results', save_name + '_' + timestamp)\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "np.savez_compressed(os.path.join(result_path, 'predictions.npz'),\n",
    "                    pred_y_list_all=pred_y_list_all)\n",
    "np.savez_compressed(os.path.join(result_path, 'auroc_score.npz'),\n",
    "                    auc_score_list_all=auc_score_list_all)\n",
    "np.savez_compressed(os.path.join(result_path, 'truths.npz'),\n",
    "                    true_y_list_all=true_y_list_all)\n",
    "np.savez_compressed(os.path.join(result_path, 'losses.npz'),\n",
    "                    losses=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temporal5",
   "language": "python",
   "name": "temporal5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
