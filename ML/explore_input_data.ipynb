{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netherlands Neurogenetics Database\n",
    "Author: Nienke Mekkes <br>\n",
    "Date: 21-Sep-2022. <br>\n",
    "Correspond: n.j.mekkes@umcg.nl <br>\n",
    "\n",
    "## Script: clinical history labeled training data: cleaning & exploration\n",
    "Objectives: load and clean training data, do some basic data exploration\n",
    "\n",
    "\n",
    "### Input files:\n",
    "- excel file with labeled training data\n",
    "\n",
    "### Output:\n",
    "- excel file with cleaned labeled training data \n",
    "- pickle file with cleaned labeled training data\n",
    "- folder with figures with basic data explorations\n",
    "\n",
    "\n",
    "#### Minimal requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "# %pip install openpyxl\n",
    "# %pip install seaborn\n",
    "# %pip install pywaffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pywaffle import Waffle\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths (user input required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_training_data = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/Final_Labeling_300_testcasusen_inclusief_pilots_Megan.xlsx\"\n",
    "save_path_files = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/training_data\"\n",
    "save_path_figures = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/figures_on_input_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(save_path_figures):\n",
    "    print('Creating figure folder....\\n')\n",
    "    os.makedirs(save_path_figures)\n",
    "    \n",
    "if not os.path.exists(save_path_files):\n",
    "    print('Creating output folder....')\n",
    "    os.makedirs(save_path_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data comes in the form of an excel file with a tab per donor. <br>\n",
    "Merge all Excel sheets together with concat, keeping the sheet names. <br>\n",
    "The sheet names are the patient identifiers. <br>\n",
    "Note, empty sheets (== donors without clinical history) are ignored by concat <br>\n",
    "\n",
    "We also load a file with general informaiton about each donor, so we can add main diagnosis information <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes some time, so run only once. Rest of script functions on data copy.\n",
    "pd_df = pd.read_excel(path_to_training_data, engine='openpyxl', index_col=[0], sheet_name=None)\n",
    "concat_training_data = pd.concat(pd_df, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = concat_training_data.copy()\n",
    "\n",
    "## some attributes have very long names, these are shortened to prevent saving error \n",
    "training_data = training_data.rename(columns={\"Hyperreflexia_and_other_reflexes\":\"Hyperreflexia_and_oth_reflexes\",\n",
    "                                              \"Unspecified_disturbed_gait_patterns\": \"Unspecified_disturbed_gait_patt\",\n",
    "                                              \"Fatique\": \"Fatigue\",\n",
    "                                              \"Lack_of_planning_organisation_overview\":\"Lack_of_planning_organis_overv\"})\n",
    "\n",
    "print('Training data has', training_data.shape[0], 'sentences and ', training_data.shape[1], 'columns.' \\\n",
    "' Non-attribute columns are:')\n",
    "non_attribute_columns = ['NBB_nr','Year_Sentence_nr','Sentence']\n",
    "for i in non_attribute_columns: print(i)\n",
    "\n",
    "# print(training_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before combining donors, training data has ', len(pd_df.keys()), 'unique NBB identifiers.')\n",
    "print('After combining donors, training data has ',len(training_data['NBB_nr'].unique()),'unique NBB identifiers.')\n",
    "before_concat = list(pd_df.keys())\n",
    "after_concat = list(training_data['NBB_nr'].unique())\n",
    "\n",
    "print('Donor files without clinical history are:',list(np.setdiff1d(before_concat,after_concat)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning part 1\n",
    "-Remove NaN sentences (5) <br>\n",
    "-Remove sentences that are just a year (63) <br>\n",
    "-Make sure that all values are a boolean of either 1 or 0. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before first round of cleaning, we have ',training_data.shape[0],'sentences.')\n",
    "\n",
    "## remove NaN\n",
    "training_data = training_data[training_data['Sentence'].notna()]\n",
    "\n",
    "## removing (year) sentences\n",
    "year = '\\(\\d+\\)$'\n",
    "training_data = training_data[lambda x: ~x['Sentence'].str.match(year)]\n",
    "\n",
    "## Shows the unique values in the 90 columns \n",
    "print('All values present in training data: ',\n",
    "      pd.unique(training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].values.ravel('K')))\n",
    "\n",
    "training_data = training_data.replace(\"TRUE \", True)\n",
    "training_data = training_data.replace(\"TRUE\", True)\n",
    "training_data = training_data.replace(\"True\", True)\n",
    "training_data = training_data.replace('False', False)\n",
    "training_data = training_data.replace(True, 1)\n",
    "training_data = training_data.replace(False, 0)\n",
    "print('All values present in training data after conversion: ',\n",
    "      pd.unique(training_data.iloc[:, 3:93].values.ravel('K')))\n",
    "\n",
    "print('After first round of cleaning, we have ',training_data.shape[0],'sentences.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning part 2\n",
    "-Add non_attribute columns with Sentence length, number of scored attributes <br>\n",
    "-Remove sentences with more than 8 attributes <br>\n",
    "-Remove sentences with fewer than 6 characters <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data[training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].sum(axis=1) < 9]\n",
    "training_data = training_data[training_data.Sentence.str.len() >= 6]\n",
    "training_data.loc[:,[i for i in list(training_data.columns) if i not in non_attribute_columns]].astype(int)\n",
    "# training_data =training_data[training_data['sentence_length'] <= 200]\n",
    "print('After second round of cleaning, we have ',training_data.shape[0],'sentences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional sentences for relabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_relabel = training_data[training_data.Sentence.str.len() >= 20]\n",
    "training_data_relabel = training_data_relabel.sample(n=1900, replace=False)\n",
    "training_data_relabel\n",
    "\n",
    "# since we cleaned up the training sentence file nicely, lets save it inbetween. this will be used to split the data.\n",
    "training_data_relabel.to_excel(f\"{save_path_files}/training_data_relabel.xlsx\")\n",
    "training_data_relabel.to_pickle(f\"{save_path_files}/training_data_relabel.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Should not be in final code, but this block creates supp. table 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comma_df = training_data#.drop(['sum_true','sentence_length'], axis=1)\n",
    "path_to_attribute_grouping = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/Clinical History - attributes grouping in categories - metadata.xlsx\"\n",
    "attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0], sheet_name='90 parameters')\n",
    "correct_names = {}\n",
    "for attr, real_name in zip(attribute_grouping.index, attribute_grouping[\"Attribute\"]):\n",
    "    if not isinstance(real_name, float):\n",
    "        correct_names[real_name] = attr\n",
    "# print(correct_names)\n",
    "comma_df = comma_df.rename(correct_names,axis=1)\n",
    "comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].replace(0, np.nan)  \n",
    "comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].replace(1, pd.Series(comma_df.columns, comma_df.columns))\n",
    "comma_df['Attribute(s)'] = comma_df.loc[:, 'Muscular weakness':'Admission to nursing home'].apply(lambda x: ','.join(x[x.notnull()]), axis = 1)\n",
    "comma_df = comma_df[['NBB_nr','Sentence','Attribute(s)']]\n",
    "display(comma_df.head(20))\n",
    "comma_df.to_excel(f\"{save_path_files}/sup4_chrono{date.today()}.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as cleaned training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we cleaned up the training sentence file nicely, lets save it inbetween. this will be used to split the data.\n",
    "training_data.to_excel(f\"{save_path_files}/cleaned_training_data.xlsx\")\n",
    "training_data.to_pickle(f\"{save_path_files}/cleaned_training_data.pkl\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dotplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 90)\n",
    "frequency = training_data.copy()\n",
    "frequency = frequency.iloc[:, -90:].sum()\n",
    "print(frequency.sort_values())\n",
    "# frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA EXPLORATION (optional)\n",
    "#### How long are our training sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = training_data_relabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sum_true\"] = df.loc[:,[i for i in list(df.columns) if i not in non_attribute_columns]].sum(axis=1)\n",
    "df['sentence_length'] = df.Sentence.str.len()\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set(rc={'figure.figsize':(15,6)},font_scale = 2) #\n",
    "sns.set_palette(\"pastel\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "## get the frequency of the previously created column sentence_length\n",
    "length_distribution = pd.DataFrame(df['sentence_length'].value_counts())\n",
    "length_distribution['x'] = length_distribution.index\n",
    "length_distribution.columns = ['nr_sentences','sentence_length']\n",
    "zero_row = {'nr_sentences':0, 'sentence_length':0}\n",
    "# length_distribution = length_distribution.append(zero_row, ignore_index=True)\n",
    "length_distribution = pd.concat([length_distribution, pd.DataFrame.from_records([zero_row])],\n",
    "                                ignore_index=True)\n",
    "length_distribution = length_distribution.sort_values(by=['sentence_length'])\n",
    "\n",
    "## plot and save\n",
    "lh = sns.barplot(x=\"sentence_length\", y=\"nr_sentences\", data=length_distribution,color='steelblue')\n",
    "lh.set(xlabel=\"Sentence length\", ylabel=\"Number of sentences\")\n",
    "plt.title(\"Sentence length distribution -- Training data\", y=1.1, fontsize = 20)\n",
    "plt.xticks(rotation=90)\n",
    "for ind, label in enumerate(lh.get_xticklabels()):\n",
    "    if ind == 0:\n",
    "        label.set_visible(True)\n",
    "    elif ind % 10 == 0: \n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "\n",
    "sns.despine(offset=10, trim=False)\n",
    "lh.spines[\"right\"].set_color(\"none\")\n",
    "lh.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentence_length_distribution_{}.png\".format(date.today()),\n",
    "#             dpi=600, bbox_inches=\"tight\")\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentence_length_distribution_{}.pdf\".format(date.today()),\n",
    "#             dpi=600, bbox_inches=\"tight\")\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many sentences have how many attributes?\n",
    "We expect that most sentences have no attribute, many sentences will have a single attribute, and a high amount of attributes for a single sentence is unlikely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get the frequency of the previously created column counting attributes per sentence\n",
    "attribute_distribution = pd.DataFrame(df['sum_true'].value_counts())\n",
    "attribute_distribution['nr_attributes'] = attribute_distribution.index\n",
    "attribute_distribution.columns = ['nr_sentences','nr_attributes']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))   \n",
    "ax =  sns.barplot(x=\"nr_attributes\", y=\"nr_sentences\", data=attribute_distribution,color = 'steelblue')\n",
    "sns.despine(offset=10, trim=False)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "\n",
    "ax.set_xlabel(\"Number of attributes per sentence\",fontsize=20)\n",
    "ax.set_ylabel(\"Sentence count\",fontsize=25)\n",
    "ax.tick_params(labelsize=20)\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentence_att_{}.pdf\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentence_att_{}.png\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More intuitive is to plot as a waffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_distribution['proportion'] = round(attribute_distribution['nr_sentences']/attribute_distribution['nr_sentences'].sum()*100,2)\n",
    "palette = sns.color_palette(\"tab20\")[0:8]\n",
    "attribute_distribution['legend'] = attribute_distribution['nr_attributes'].astype(str) +': ' + attribute_distribution['proportion'].astype(str) + '%'\n",
    "test = pd.Series(attribute_distribution.nr_sentences.values,index=attribute_distribution.nr_attributes).to_dict()\n",
    "\n",
    "fig = plt.figure(\n",
    "    FigureClass=Waffle, \n",
    "    rows=30, \n",
    "    values=list(attribute_distribution.nr_sentences/13),\n",
    "    colors = palette,\n",
    "    figsize=(10, 8),\n",
    "        legend={'labels':list(attribute_distribution.legend),\n",
    "            'loc': 'upper right', 'bbox_to_anchor': (1.1, -0.07), \n",
    "            'ncol': 5, \n",
    "            'framealpha': 0,\n",
    "            'title':'Attributes per sentence',\n",
    "            'title_fontsize':8,\n",
    "            'fontsize': 6\n",
    "               }    \n",
    ")\n",
    "# plt.savefig(save_path_figures + \"/training_data_waffle_sentence_att_{}.pdf\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.savefig(save_path_figures + \"/training_data_waffle_sentence_att_{}.png\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the relationship between sentence length and number of attributes?\n",
    "We expect that longer sentences have more attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x='sum_true', y='sentence_length', data=df,kind=\"violin\",inner=None, palette='Blues')\n",
    "plt.xlabel(\"Number of attributes\")\n",
    "plt.ylabel(\"Sentence length\")\n",
    "plt.tick_params(labelsize=10)\n",
    "# plt.savefig(save_path_figures + \"/training_data_violin_sentence_length_attributes_{}.png\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.savefig(save_path_figures + \"/training_data_violin_sentence_length_attributes_{}.pdf\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training set: sentence distribution\n",
    "We expect to find differences in how many sentences each donor has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "sns.set(rc={'figure.figsize':(30,6)},font_scale = 2) #\n",
    "sns.set_palette(\"pastel\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "## NBB nr frequency df\n",
    "sentences_per_donor = pd.DataFrame(df['NBB_nr'].value_counts())\n",
    "sentences_per_donor['x'] = sentences_per_donor.index\n",
    "sentences_per_donor.columns = ['nr_sentences','NBB_nr']\n",
    "\n",
    "## plot\n",
    "ax = sns.barplot(x=\"NBB_nr\", y=\"nr_sentences\", data=sentences_per_donor, color='steelblue')\n",
    "plt.xlabel(\"Donor ID\")\n",
    "plt.ylabel(\"# of sentences\")\n",
    "\n",
    "plt.tick_params(labelsize=8)\n",
    "ax.spines[\"right\"].set_color(\"none\")\n",
    "ax.spines[\"top\"].set_color(\"none\")\n",
    "plt.xticks(rotation=90)\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentences_per_donor_{}.png\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.savefig(save_path_figures + \"/training_data_sentences_per_donor_{}.pdf\".format(date.today()),\n",
    "#             bbox_inches=\"tight\",dpi=600)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
