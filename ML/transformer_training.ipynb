{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netherlands Neurogenetics Database\n",
    "Author: Nienke Mekkes <br>\n",
    "Date: 21-Sep-2022. <br>\n",
    "Correspond: n.j.mekkes@umcg.nl <br>\n",
    "\n",
    "## Script: clinical history NLP models: PubMedBERT\n",
    "Objectives: optimization of PubMedBERT based model <br>\n",
    "Based on: \n",
    "\n",
    "\n",
    "### Input files:\n",
    "- File (excel or pickle) containing the train&val data\n",
    "- File (excel or pickle) containing the clean training data (before split_data.ipynb), to extract attribute names\n",
    "- File containing the attribute metadata, to plot official attribute names\n",
    "\n",
    "### Output:\n",
    "- Folder containing: <br>\n",
    "    - optuna study in the form of a .db and a .pkl file\n",
    "    - csv file with performance metrics per attribute for all trials\n",
    "    - csv file with performance of the best trial only\n",
    "    - figure folder with analyses\n",
    "\n",
    "\n",
    "\n",
    "#### Minimal requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU, 3.82 werkt (3.7.4 niet)\n",
    "# %pip install optuna\n",
    "# %pip install nltk\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install kaleido\n",
    "# %pip install adjustText\n",
    "# %pip install simpletransformers\n",
    "# %pip install torch\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths (user input required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_trainval_xlsx =  \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/trainval_data.xlsx\"\n",
    "path_to_trainval_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/trainval_data.pkl\"\n",
    "# path_to_test_xlsx =  \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/test_data.xlsx\"\n",
    "path_to_test_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/test_data.pkl\"\n",
    "# path_to_cleaned_training_data_xlsx = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/cleaned_training_data.xlsx\"\n",
    "path_to_cleaned_training_data_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/cleaned_training_data.pkl\"\n",
    "path_to_attribute_grouping = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/input_data/Clinical History - attributes grouping in categories - metadata_oct.xlsx\"\n",
    "\n",
    "path_to_additional_functions = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/scripts\"\n",
    "study_name = 'test'\n",
    "used_model = 'PubMedBERT'\n",
    "model_save_location = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/nlp_models\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import optuna\n",
    "import logging,sys\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "import plotly\n",
    "import kaleido\n",
    "from optuna.visualization import plot_param_importances, plot_parallel_coordinate, plot_optimization_history\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import sys\n",
    "sys.path.insert(1, path_to_additional_functions)\n",
    "from helper_functions import  scatter_plot,plot_trials,analysis_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel, MultiLabelClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(MultiLabelClassificationModel.__version__)\n",
    "import simpletransformers\n",
    "\n",
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data \n",
    "Either from pickle or from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = f\"{model_save_location}/{used_model}/{study_name}\"\n",
    "storage_name = \"sqlite:///{}/{}.db\".format(model_save_path,study_name)\n",
    "if not os.path.exists(model_save_path):\n",
    "    print('Creating model folder ',model_save_path)\n",
    "    os.makedirs(model_save_path)\n",
    "    \n",
    "    \n",
    "save_path_figures = '{}/figures'.format(model_save_path)\n",
    "if not os.path.exists(save_path_figures):\n",
    "    print('Creating model figure folder ',save_path_figures)\n",
    "    os.makedirs(save_path_figures)\n",
    "\n",
    "\n",
    "\n",
    "# trainval = pd.read_excel(path_to_trainval_xlsx, engine='openpyxl', index_col=[0])\n",
    "with open(path_to_trainval_pkl,\"rb\") as file:\n",
    "    trainval = pickle.load(file)\n",
    "# display(trainval)\n",
    "# trainval[\"labels\"] = [eval(row[\"labels\"]) for index, row in trainval.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load cleaned data file to get column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_train = pd.read_excel(path_to_cleaned_training_data_xlsx, engine='openpyxl', index_col=[0])\n",
    "with open(path_to_cleaned_training_data_pkl,\"rb\") as file:\n",
    "    cleaned_train = pickle.load(file)\n",
    "display(cleaned_train)\n",
    "\n",
    "non_attribute_columns = ['NBB_nr','Year_Sentence_nr','Sentence']\n",
    "attributes = [col for col in cleaned_train.columns if col not in non_attribute_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna has an objective function which returns a value. Optuna then tries to maximize that value, where it tries new parameter combinations each trial. In our case the value we want to optimize is performance of all 90 attributes, averaged over the 5 folds. This is the average 5 fold micro F1 score. Creating the model and optimization itself is quite straightforward. For analysis purposes however, we also want to save other performance metrics for all trials into a csv file. We do not save the intermediate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    counter = 1\n",
    "    dataframe_list = []\n",
    "    \n",
    "    ## set up split\n",
    "    X = trainval[['text']].to_numpy() \n",
    "    Y = pd.DataFrame(trainval['labels'])\n",
    "    Y = pd.DataFrame(Y['labels'].to_list())\n",
    "    Y = Y.to_numpy() \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True,random_state=0)\n",
    "    \n",
    "    ## Split the train&val into multiple train and val sets.\n",
    "    for train_index, val_index in mskf.split(X, Y):\n",
    "        x_train, x_val = X[train_index], X[val_index] \n",
    "        y_train, y_val = Y[train_index], Y[val_index]\n",
    "        train = pd.DataFrame(x_train,columns=['text']) \n",
    "        val = pd.DataFrame(x_val,columns=['text'])\n",
    "        train['labels'] = y_train.tolist()\n",
    "        val['labels'] = y_val.tolist()\n",
    "        \n",
    "        print(\"Trial nr:\",trial.number,\"Fold number:\",counter,\"\\nTRAIN row numbers:\", train_index, \"\\nVAL row numbers:\", val_index)\n",
    "        \n",
    "        print(f\"nr of sentences in train: {len(x_train)}, or {len(x_train)/(len(x_train)+len(x_val)):.2f}%\\n\" \\\n",
    "              f\"nr of sentences in val: {len(x_val)}, or {len(x_val)/(len(x_train)+len(x_val)):.2f}% \\n\")\n",
    "        \n",
    "        ## to optimize\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-4)\n",
    "#         th = trial.suggest_float(\"th\", 0.4, 0.7)\n",
    "        ep = trial.suggest_int(\"ep\", 1, 2)#20,35\n",
    "        model_args_bert = { \"do_lower_case\": True, # for uncased models\n",
    "               \"fp16\": True,#speeds up, but risk under/overflow\n",
    "               \"learning_rate\": lr, # candidate for optimalisation\n",
    "               \"manual_seed\": 2,\n",
    "               \"max_seq_length\": 300, #Chosen such that most samples are not truncated. Increasing the sequence length significantly affects the memory consumption of the model, so it s usually best to keep it as short as possible (ideally without truncating the input sequences).\n",
    "               \"num_train_epochs\": ep, # option for optimalisation\n",
    "              #\"optimizer\": \"Adafactor\", # option for optimalisation\n",
    "               \"output_dir\": model_save_path + '/Optuna',\n",
    "               \"overwrite_output_dir\": True,\n",
    "               \"reprocess_input_data\" : True, #default true, input data will be reprocessed even if a cached file of the input data exists.\n",
    "               \"save_eval_checkpoints\":False,\n",
    "               \"save_model_every_epoch\":False,\n",
    "               \"save_optimizer_and_scheduler\":False,\n",
    "               \"save_steps\": -1,\n",
    "               \"silent\":False,\n",
    "              #\"scheduler\": \"linear_schedule_with_warmup\",  # option for optimalisation\n",
    "              #\"sliding_window\": True # not supported, but advised? # option for optimalisation\n",
    "               \"train_batch_size\": 16,  \n",
    "               \"use_multiprocessing\": True, #speeds up,may be unstable, has some issues reported with t5\n",
    "               \"wandb_project\": None,#\"pubmed_wandnm\",\n",
    "                \"wandb_kwargs\": {\"mode\":\"disabled\"},\n",
    "               \"threshold\":0.6}\n",
    "        model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "                                              \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", ## \"modelname from huggingface\"\n",
    "                                              args=model_args_bert,\n",
    "                                              use_cuda=False,#True,\n",
    "                                              num_labels=90)  \n",
    "        model.train_model(train)\n",
    "        \n",
    "        ## y_val: the true labels in numpy array form\n",
    "        y_val = val[\"labels\"]\n",
    "        y_val = pd.DataFrame(y_val) \n",
    "        y_val = pd.DataFrame(y_val['labels'].to_list())\n",
    "        y_val = y_val.to_numpy()\n",
    "        \n",
    "        ## the predicted labels in numpy array form\n",
    "        sentences = val[\"text\"].values\n",
    "        sentences = [str(i) for i in list(sentences)]\n",
    "        predictions, raw_outputs = model.predict(list(sentences))\n",
    "        y_val_predicted_labels_pubmedbert = np.array(predictions)\n",
    "         \n",
    "        all_metrics = classification_report(y_val, y_val_predicted_labels_pubmedbert,\n",
    "                                            target_names=attributes,digits=3,output_dict=True)   \n",
    "        all_metrics_df = pd.DataFrame(all_metrics).transpose()\n",
    "        all_metrics_df['Trial'] = trial.number\n",
    "        all_metrics_df['Fold'] = counter\n",
    "        dataframe_list.append(all_metrics_df)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    performance_all_folds = pd.concat(dataframe_list)\n",
    "    performance_all_folds = performance_all_folds.reset_index()\n",
    "    performance_all_folds = performance_all_folds.rename(columns={\"index\": \"Attribute\"})\n",
    "    average_performance = performance_all_folds.drop('Fold',axis=1)\n",
    "    average_performance = average_performance.groupby('Attribute').mean()\n",
    "    display(performance_all_folds)\n",
    "    micro_f1_score = average_performance.loc['micro avg']['f1-score'] \n",
    "    \n",
    "    ## add to csv file containing all trials for all folds\n",
    "    with open('{}/{}.csv'.format(model_save_path,study_name),'a') as f:\n",
    "        performance_all_folds.to_csv(f, header = False,index=False)\n",
    "    print(micro_f1_score)\n",
    "    print('-----------------------')\n",
    "    \n",
    "    \n",
    "    return micro_f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study = optuna.create_study(study_name=study_name,direction='maximize',load_if_exists=True, storage=storage_name)\n",
    "f = model_save_path + '/' + study_name + '.csv'\n",
    "file_exists = os.path.isfile(f)\n",
    "if not file_exists:\n",
    "    print('creating csv file {} to save model performance'.format(f))\n",
    "    with open(f, 'w') as f:\n",
    "        writer = csv.writer(f,delimiter=',', lineterminator='\\n')\n",
    "        writer.writerow(['Attribute','Precision','Recall','F1','Support','Trial','Fold'])\n",
    "        f.close()\n",
    "        \n",
    "study = optuna.load_study(study_name=study_name, storage=storage_name)#,\"maximize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize Optuna\n",
    "IMPORTANT: if you have already finished an optuna trial and are pleased with the results, and do not want additional trials, do not run this block. <br>\n",
    "Typically I run for 30 Trials, we do not see improvement after 30 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "    study.optimize(objective, n_trials=2,n_jobs=2)\n",
    "\n",
    "## additional, store using joblib:\n",
    "savename = '{}/{}.pkl'.format(model_save_path,study_name)\n",
    "joblib.dump(study, savename)\n",
    "\n",
    "print('finished optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick overview of Optuna results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best trial:\n",
    "print(f\"The best trial is : \\n{study.best_trial} \\n\")\n",
    "\n",
    "# Getting the best score:\n",
    "print(f\"The highest f1 value is : \\n{study.best_value}\\n\")\n",
    "\n",
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures to illustrate optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: Hyperparameter importance\n",
    "Which hyperparameter has the most influence on model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pai = plot_param_importances(study)\n",
    "pai.update_layout(\n",
    "    title='{} Hyperparameter importance'.format(used_model),\n",
    "    xaxis_title='Importance for F1-score',\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pai.update_xaxes(range=[0,1])\n",
    "pai.write_image(save_path_figures + \"/{}_{}_parameter_importance.png\".format(used_model,study_name))\n",
    "pai.write_image(save_path_figures + \"/{}_{}_parameter_importance.pdf\".format(used_model,study_name))\n",
    "pai.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2: optuna visualisation plot optimization\n",
    "Improvement over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oph = plot_optimization_history(study,target_name='F1-score')\n",
    "oph.update_layout(\n",
    "    title='{} Optimization history'.format(used_model),\n",
    "    xaxis_title='Number of trials',\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "oph.update_yaxes(range=[0,1])\n",
    "oph.write_image(save_path_figures + \"/{}_{}_optimization_history.png\".format(used_model,study_name))\n",
    "oph.write_image(save_path_figures + \"/{}_{}_optimization_history.pdf\".format(used_model,study_name))\n",
    "oph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3: optuna visualisation parallel coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = plot_parallel_coordinate(study,target_name='F1-score')\n",
    "\n",
    "pac.update_layout(\n",
    "    title='{} Parallel Coordinate Plot'.format(used_model),\n",
    "    yaxis_title=\"F1-score\",\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        pad=40\n",
    "    ),\n",
    "    autosize=False,\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=16,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pac.write_image(save_path_figures + \"/{}_{}_parallel_coordinate.png\".format(used_model,study_name))\n",
    "pac.write_image(save_path_figures + \"/{}_{}_parallel_coordinate.pdf\".format(used_model,study_name))\n",
    "pac.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4: All trials; plot improvement over trials (f1 and precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_number,best_trial,f1_precision_long,micro_F1,micro_Precision = analysis_performance(model_save_path,study_name,used_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(f1_precision_long,\n",
    "            save_path_figures,\n",
    "            used_model,\n",
    "            study_name,\n",
    "            best_trial_number,\n",
    "            metric='F1',\n",
    "            pal='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(f1_precision_long,\n",
    "            save_path_figures,\n",
    "            used_model,\n",
    "            study_name,\n",
    "            best_trial_number,\n",
    "            metric='Precision',\n",
    "            pal='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute scatter\n",
    "We have selected the best trial. How do the different attributes perform under this trial? Some might be bad. We plot the precision against the F1 score for all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0], sheet_name='90 parameters')\n",
    "display(attribute_grouping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = {}\n",
    "for attr, real_name in zip(attribute_grouping.index, attribute_grouping[\"Attribute\"]):\n",
    "    if not isinstance(real_name, float):\n",
    "        correct_names[real_name] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = best_trial.rename(index=correct_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select some attributes that we want to plot with their text label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_attributes = ['Unspecified_disturbed_gait_patt',\n",
    "'Loss_of_sympathy_empathy',\n",
    "'Fasciculations',\n",
    "# 'Psychiatric_admissions',\n",
    "'Changed_moods_emotions',\n",
    "'Bradyphrenia',\n",
    "'Head_turning_sign',\n",
    "# 'Communication_problems',\n",
    "# 'Decreased_motor_skills',\n",
    "'Language_impairment',\n",
    "# 'Positive_sensory_symptoms',\n",
    "'Facade_behavior',\n",
    "# 'Impaired_comprehension',\n",
    "'Changed_behavior_personality',\n",
    "'Frontal_release_signs',\n",
    "'Vivid_dreaming',\n",
    "'Loss_of_sympathy_empathy'\n",
    "]\n",
    "\n",
    "fancy_bad_attributes = [correct_names.get(item,item)  for item in bad_attributes]\n",
    "print(fancy_bad_attributes)\n",
    "\n",
    "good_attributes = [\n",
    "# 'Parkinsonism',\n",
    "'Memory_impairment',\n",
    "'Mobility_problems',\n",
    "'Fatigue',\n",
    "'Depressed_mood'\n",
    "# 'Psychosis',\n",
    "'Agitation',\n",
    "# 'Fatigue',\n",
    "]\n",
    "\n",
    "fancy_good_attributes = [correct_names.get(item,item)  for item in good_attributes]\n",
    "print(fancy_good_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot for optuna best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = best_trial.drop([\"micro avg\", \"macro avg\", \"weighted avg\",\"samples avg\"],errors='ignore')\n",
    "scatter_plot(best_trial, \n",
    "             used_model,\n",
    "             study_name,\n",
    "             fancy_good_attributes,\n",
    "             fancy_bad_attributes,\n",
    "             metrics='Precision_F1',\n",
    "             printf1=round(micro_F1,5),\n",
    "             printprec=round(micro_Precision,5),\n",
    "             trialname=best_trial_number,\n",
    "             wheretosave=save_path_figures,\n",
    "             val_or_test='Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on train&val data with best parameters, evaluate on hold-out test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on the combined training and validation data. we test on the kept apart testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_test_pkl,\"rb\") as file:\n",
    "    test = pickle.load(file)\n",
    "\n",
    "# test[\"labels\"] = [eval(row[\"labels\"]) for index, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args_PubMedBERT = { \"do_lower_case\": True, # for uncased models\n",
    "       \"fp16\": True,#speeds up, but risk under/overflow\n",
    "       \"learning_rate\": study.trials[best_trial_number].params['lr'], # candidate for optimalisation\n",
    "       \"manual_seed\": 2,\n",
    "       \"max_seq_length\": 300, #Chosen such that most samples are not truncated. Increasing the sequence length significantly affects the memory consumption of the model, so it s usually best to keep it as short as possible (ideally without truncating the input sequences).\n",
    "       \"num_train_epochs\": study.trials[best_trial_number].params['ep'], # option for optimalisation\n",
    "      #\"optimizer\": \"Adafactor\", # option for optimalisation\n",
    "       \"output_dir\": path_final,\n",
    "       \"overwrite_output_dir\": True,\n",
    "       \"reprocess_input_data\" : True, #default true, input data will be reprocessed even if a cached file of the input data exists.\n",
    "       \"save_eval_checkpoints\":False,\n",
    "       \"save_model_every_epoch\":False,\n",
    "       \"save_optimizer_and_scheduler\":False,\n",
    "       \"save_steps\": -1,\n",
    "       \"silent\":False,\n",
    "      #\"scheduler\": \"linear_schedule_with_warmup\",  # option for optimalisation\n",
    "      #\"sliding_window\": True # not supported, but advised? # option for optimalisation\n",
    "       \"train_batch_size\": 16,  \n",
    "       \"use_multiprocessing\": True, #speeds up,may be unstable, has some issues reported with t5\n",
    "       \"wandb_project\": None,\n",
    "        \"wandb_kwargs\": {\"mode\":\"disabled\"},\n",
    "       \"threshold\":0.6\n",
    " }\n",
    "\n",
    "## when training for the first time\n",
    "model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "                                      \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", ## \"modelname from huggingface\"\n",
    "                                      args=model_args_PubMedBERT,\n",
    "                                      use_cuda=True,\n",
    "                                      num_labels=90)\n",
    "\n",
    "model.train_model(trainval)\n",
    "\n",
    "# ## when loading already trained model\n",
    "# model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "#                                       path_final, ## \"modelname from huggingface\"\n",
    "#                                       args=model_args_PubMedBERT,\n",
    "#                                       use_cuda=True,#True,\n",
    "#                                       num_labels=90) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the truths\n",
    "y_test = test['labels']\n",
    "y_test = pd.DataFrame(y_test) \n",
    "y_test = pd.DataFrame(y_test['labels'].to_list())\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "# Create a list with the text to predict (parsed sentences)\n",
    "sentences = test['text'].values#val[\"text\"].values\n",
    "sentences = [str(i) for i in list(sentences)]\n",
    "predictions, raw_outputs = model.predict(list(sentences))\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "test_report = classification_report(y_test, predictions, target_names=attributes,\n",
    "                                       digits=3,output_dict=True)\n",
    "test_report_df = pd.DataFrame(test_report).transpose()\n",
    "test_report_df.columns=['Precision','Recall','F1','Support']\n",
    "F1 = test_report_df.loc['micro avg']['Precision'] #!4\n",
    "Precision = test_report_df.loc['micro avg']['F1']\n",
    "print('F1-score micro: ', F1)\n",
    "print('Precision micro: ', Precision)\n",
    "\n",
    "display(test_report_df)\n",
    "\n",
    "## save the performance\n",
    "test_report_df.to_csv(model_save_path +'/{}_{}_test_performance.csv'.format(used_model,study_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the sentences with their truth label and prediction label, for investigation\n",
    "truths = pd.DataFrame(y_test,columns=attributes).add_suffix('_Truth')\n",
    "predictions_df = pd.DataFrame(predictions,columns=attributes).add_suffix('_Prediction')\n",
    "sentences = pd.concat([truths,predictions_df], axis=1)\n",
    "sentences = sentences.reindex(sorted(sentences.columns), axis=1)\n",
    "sentences.insert(loc=0, column='Sentence', value=test['text'])\n",
    "\n",
    "display(sentences)\n",
    "# sentences.to_csv(model_save_path +'/{}_{}_test_sentences.csv'.format(used_model,study_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report_df['pass_fail'] = np.where(((test_report_df.F1 >= 0.8) | (test_report_df.Precision >= 0.8) ),'pass', 'fail')\n",
    "test_report_df = test_report_df.drop([\"micro avg\", \"macro avg\", \"weighted avg\",\"samples avg\"],errors='ignore')\n",
    "display(test_report_df)\n",
    "scatter_plot(test_report_df, \n",
    "             used_model,\n",
    "             study_name,\n",
    "             fancy_good_attributes,\n",
    "             fancy_bad_attributes,\n",
    "             metrics='Precision_F1',\n",
    "             printf1=round(F1,5),\n",
    "             printprec=round(Precision,5),\n",
    "             trialname='',\n",
    "             wheretosave=save_path_figures,\n",
    "             val_or_test='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
