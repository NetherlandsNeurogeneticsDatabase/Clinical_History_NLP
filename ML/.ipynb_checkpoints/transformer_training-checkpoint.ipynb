{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netherlands Neurogenetics Database\n",
    "Author: Nienke Mekkes <br>\n",
    "Date: 21-Sep-2022. <br>\n",
    "Correspond: n.j.mekkes@umcg.nl <br>\n",
    "\n",
    "## Script: clinical history NLP models: PubMedBERT\n",
    "Objectives: optimization of PubMedBERT based model <br>\n",
    "Based on: \n",
    "\n",
    "\n",
    "### Input files:\n",
    "- File (excel or pickle) containing the train&val data\n",
    "- File (excel or pickle) containing the clean training data (before split_data.ipynb), to extract attribute names\n",
    "- File containing the attribute metadata, to plot official attribute names\n",
    "\n",
    "### Output:\n",
    "- Folder containing: <br>\n",
    "    - optuna study in the form of a .db and a .pkl file\n",
    "    - csv file with performance metrics per attribute for all trials\n",
    "    - csv file with performance of the best trial only\n",
    "    - figure folder with analyses\n",
    "\n",
    "\n",
    "\n",
    "#### Minimal requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU, 3.82 werkt (3.7.4 niet)\n",
    "# %pip install optuna\n",
    "# %pip install nltk\n",
    "# %pip install scikit-learn\n",
    "# %pip install plotly\n",
    "# %pip install kaleido\n",
    "# %pip install adjustText\n",
    "# %pip install simpletransformers\n",
    "# %pip install torch\n",
    "# %pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paths (user input required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_trainval_xlsx =  \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/trainval_data.xlsx\"\n",
    "path_to_trainval_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/trainval_data.pkl\"\n",
    "# path_to_test_xlsx =  \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/test_data.xlsx\"\n",
    "path_to_test_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/test_data.pkl\"\n",
    "# path_to_cleaned_training_data_xlsx = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/cleaned_training_data.xlsx\"\n",
    "path_to_cleaned_training_data_pkl = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/training_data/cleaned_training_data.pkl\"\n",
    "path_to_attribute_grouping = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/input_data/Clinical History - attributes grouping in categories - metadata_oct.xlsx\"\n",
    "\n",
    "path_to_additional_functions = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/scripts\"\n",
    "study_name = 'test'\n",
    "used_model = 'PubMedBERT'\n",
    "model_save_location = \"/home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/nlp_models\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import optuna\n",
    "import logging,sys\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import csv\n",
    "\n",
    "import plotly\n",
    "import kaleido\n",
    "from optuna.visualization import plot_param_importances, plot_parallel_coordinate, plot_optimization_history\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import sys\n",
    "sys.path.insert(1, path_to_additional_functions)\n",
    "from helper_functions import  scatter_plot,plot_trials,analysis_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel, MultiLabelClassificationArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==0.12.0\n",
      "adjustText==0.7.3\n",
      "aiohttp==3.8.3\n",
      "aiosignal==1.2.0\n",
      "alembic==1.8.1\n",
      "algorithmx==2.0.3\n",
      "altair==4.2.0\n",
      "argon2-cffi @ file:///tmp/build/80754af9/argon2-cffi_1596828452693/work\n",
      "asn1crypto==0.24.0\n",
      "astunparse==1.6.3\n",
      "async-generator==1.10\n",
      "async-timeout==4.0.2\n",
      "asynctest==0.13.0\n",
      "attrs @ file:///tmp/build/80754af9/attrs_1598374659300/work\n",
      "autopage==0.5.1\n",
      "backcall==0.2.0\n",
      "backports.zoneinfo==0.2.1\n",
      "bleach==3.1.5\n",
      "blinker==1.5\n",
      "bokeh @ file:///tmp/build/80754af9/bokeh_1598903502831/work\n",
      "botocore==1.23.10\n",
      "bz2file==0.98\n",
      "cachetools==4.2.1\n",
      "certifi==2022.9.24\n",
      "certipy==0.1.3\n",
      "cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1613413867554/work\n",
      "chardet==3.0.4\n",
      "charset-normalizer==2.1.1\n",
      "click==8.1.3\n",
      "cliff==3.10.1\n",
      "cmaes==0.8.2\n",
      "cmd2==2.4.2\n",
      "colorlog==6.7.0\n",
      "commonmark==0.9.1\n",
      "conda==4.13.0\n",
      "conda-package-handling==1.3.11\n",
      "cryptography==2.7\n",
      "cutadapt==1.18\n",
      "cycler==0.10.0\n",
      "Cython==0.29.28\n",
      "datasets==2.6.1\n",
      "decorator==4.4.2\n",
      "defusedxml==0.6.0\n",
      "dill==0.3.5.1\n",
      "docker-pycreds==0.4.0\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "filelock==3.8.0\n",
      "flatbuffers==1.12\n",
      "fontawesomefree==6.2.0\n",
      "frozenlist==1.3.1\n",
      "fsspec==2022.10.0\n",
      "fuzzywuzzy==0.18.0\n",
      "gast==0.3.3\n",
      "gitdb==4.0.9\n",
      "GitPython==3.1.29\n",
      "glob2==0.7\n",
      "google-api-core==2.2.2\n",
      "google-auth==1.29.0\n",
      "google-auth-oauthlib==0.4.4\n",
      "google-crc32c==1.3.0\n",
      "google-pasta==0.2.0\n",
      "googleapis-common-protos==1.53.0\n",
      "grpcio==1.32.0\n",
      "gsport==1.6.1\n",
      "h5py==2.10.0\n",
      "huggingface-hub==0.10.1\n",
      "idna==2.8\n",
      "imbalanced-learn==0.9.0\n",
      "importlib-metadata @ file:///tmp/build/80754af9/importlib-metadata_1593446408836/work\n",
      "importlib-resources==5.10.0\n",
      "ipykernel @ file:///tmp/build/80754af9/ipykernel_1596206598566/work/dist/ipykernel-5.3.4-py3-none-any.whl\n",
      "ipython @ file:///tmp/build/80754af9/ipython_1598883837425/work\n",
      "ipython_genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "iterative-stratification==0.1.7\n",
      "jedi @ file:///tmp/build/80754af9/jedi_1596490743326/work\n",
      "Jinja2==2.11.2\n",
      "jmespath==0.10.0\n",
      "joblib==1.2.0\n",
      "json5==0.9.5\n",
      "jsonpickle @ file:///home/conda/feedstock_root/build_artifacts/jsonpickle_1618250231429/work\n",
      "jsonschema @ file:///tmp/build/80754af9/jsonschema_1594363551272/work\n",
      "jupyter-client==6.1.7\n",
      "jupyter-core==4.6.3\n",
      "jupyterhub==1.0.0\n",
      "jupyterlab==2.2.6\n",
      "jupyterlab-server==1.2.0\n",
      "kaleido==0.2.1\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.2.0\n",
      "libarchive-c==2.8\n",
      "Mako==1.1.3\n",
      "mamba @ file:///home/conda/feedstock_root/build_artifacts/mamba_1615553193471/work\n",
      "Markdown==3.3.4\n",
      "MarkupSafe @ file:///tmp/build/80754af9/markupsafe_1594371495811/work\n",
      "matplotlib @ file:///tmp/build/80754af9/matplotlib-base_1597876320876/work\n",
      "mistune @ file:///tmp/build/80754af9/mistune_1594373098390/work\n",
      "mkl-fft==1.1.0\n",
      "mkl-random==1.1.1\n",
      "mkl-service==2.3.0\n",
      "multidict==6.0.2\n",
      "multiprocess==0.70.13\n",
      "natsort @ file:///home/conda/feedstock_root/build_artifacts/natsort_1611580267114/work\n",
      "nbconvert @ file:///tmp/build/80754af9/nbconvert_1594376811065/work\n",
      "nbformat==5.0.7\n",
      "nbgitpuller==0.7.2\n",
      "nbresuse==0.3.6\n",
      "ncls @ file:///opt/conda/conda-bld/ncls_1623263161568/work\n",
      "NetComp==0.2.3\n",
      "networkx==1.11\n",
      "nltk==3.7\n",
      "notebook @ file:///tmp/build/80754af9/notebook_1596838645154/work\n",
      "nteract-on-jupyter==2.1.3\n",
      "numpy==1.19.5\n",
      "oauthlib==3.1.0\n",
      "olefile==0.46\n",
      "openpyxl @ file:///home/conda/feedstock_root/build_artifacts/openpyxl_1632332968451/work\n",
      "opt-einsum==3.3.0\n",
      "optuna==3.0.3\n",
      "packaging==21.3\n",
      "pamela==1.0.0\n",
      "pandas==1.2.3\n",
      "pandocfilters==1.4.2\n",
      "parso==0.7.1\n",
      "pathtools==0.1.2\n",
      "patsy==0.5.1\n",
      "pbr==5.11.0\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1594383317248/work\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1594384075987/work\n",
      "Pillow @ file:///tmp/build/80754af9/pillow_1594307325547/work\n",
      "plotly==5.10.0\n",
      "prettytable==3.4.1\n",
      "prometheus-client==0.8.0\n",
      "promise==2.3\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1598885458782/work\n",
      "protobuf==3.15.8\n",
      "psutil==5.7.2\n",
      "ptyprocess==0.6.0\n",
      "pyarrow==9.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pydeck==0.8.0b4\n",
      "Pygments==2.6.1\n",
      "Pympler==1.0.1\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.7\n",
      "pyperclip==1.8.2\n",
      "pyranges==0.0.55\n",
      "pyrle==0.0.33\n",
      "pyrsistent==0.16.0\n",
      "PySocks==1.7.0\n",
      "python-dateutil==2.8.1\n",
      "python-editor==1.0.4\n",
      "pytz==2020.1\n",
      "pytz-deprecation-shim==0.1.0.post0\n",
      "pyvis @ file:///home/conda/feedstock_root/build_artifacts/pyvis_1614972133305/work\n",
      "pywaffle==1.1.0\n",
      "PyYAML==5.3.1\n",
      "pyzmq==19.0.2\n",
      "regex==2021.11.10\n",
      "requests==2.22.0\n",
      "requests-oauthlib==1.3.0\n",
      "responses==0.18.0\n",
      "rich==12.6.0\n",
      "rsa==4.7.2\n",
      "ruamel_yaml==0.15.46\n",
      "s3transfer==0.5.0\n",
      "scikit-learn==1.0.2\n",
      "scikit-multilearn==0.2.0\n",
      "scipy==1.7.3\n",
      "seaborn @ file:///tmp/build/80754af9/seaborn_1629307859561/work\n",
      "semver==2.13.0\n",
      "Send2Trash==1.5.0\n",
      "sentencepiece==0.1.97\n",
      "sentry-sdk==1.9.0\n",
      "seqeval==1.2.2\n",
      "setproctitle==1.3.2\n",
      "shortuuid==1.0.9\n",
      "simpletransformers==0.63.9\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1590081179328/work\n",
      "sklearn==0.0\n",
      "smmap==5.0.0\n",
      "sorted-nearest==0.0.33\n",
      "SQLAlchemy==1.3.19\n",
      "statsmodels @ file:///home/conda/feedstock_root/build_artifacts/statsmodels_1612273600586/work\n",
      "stevedore==3.5.2\n",
      "streamlit==1.13.0\n",
      "tabulate @ file:///home/conda/feedstock_root/build_artifacts/tabulate_1614001031686/work\n",
      "tenacity==8.1.0\n",
      "tensorboard==2.5.0\n",
      "tensorboard-data-server==0.6.0\n",
      "tensorboard-plugin-wit==1.8.0\n",
      "tensorflow==2.4.1\n",
      "tensorflow-estimator==2.4.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.8.3\n",
      "testpath==0.4.4\n",
      "threadpoolctl==3.1.0\n",
      "tokenizers==0.13.1\n",
      "toml==0.10.2\n",
      "toolz==0.12.0\n",
      "torch==1.12.1\n",
      "tornado==6.0.4\n",
      "tqdm==4.64.1\n",
      "traitlets==5.0.3\n",
      "transformers==4.23.1\n",
      "typing_extensions==4.4.0\n",
      "tzdata==2021.5\n",
      "tzlocal==4.1\n",
      "urllib3==1.25.11\n",
      "validators==0.20.0\n",
      "wandb==0.13.4\n",
      "watchdog==2.1.9\n",
      "wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.2.1\n",
      "Werkzeug==1.0.1\n",
      "widgetsnbextension==3.5.1\n",
      "wrapt==1.12.1\n",
      "xlrd==1.2.0\n",
      "XlsxWriter @ file:///home/conda/feedstock_root/build_artifacts/xlsxwriter_1645975313810/work\n",
      "xopen==0.7.3\n",
      "xxhash==3.1.0\n",
      "yarl==1.8.1\n",
      "zipp==3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# print(MultiLabelClassificationModel.__version__)\n",
    "import simpletransformers\n",
    "\n",
    "%pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data \n",
    "Either from pickle or from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model folder  /home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/nlp_models/PubMedBERT/test\n",
      "Creating model figure folder  /home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/nlp_models/PubMedBERT/test/figures\n"
     ]
    }
   ],
   "source": [
    "model_save_path = f\"{model_save_location}/{used_model}/{study_name}\"\n",
    "storage_name = \"sqlite:///{}/{}.db\".format(model_save_path,study_name)\n",
    "if not os.path.exists(model_save_path):\n",
    "    print('Creating model folder ',model_save_path)\n",
    "    os.makedirs(model_save_path)\n",
    "    \n",
    "    \n",
    "save_path_figures = '{}/figures'.format(model_save_path)\n",
    "if not os.path.exists(save_path_figures):\n",
    "    print('Creating model figure folder ',save_path_figures)\n",
    "    os.makedirs(save_path_figures)\n",
    "\n",
    "\n",
    "\n",
    "# trainval = pd.read_excel(path_to_trainval_xlsx, engine='openpyxl', index_col=[0])\n",
    "with open(path_to_trainval_pkl,\"rb\") as file:\n",
    "    trainval = pickle.load(file)\n",
    "# display(trainval)\n",
    "# trainval[\"labels\"] = [eval(row[\"labels\"]) for index, row in trainval.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load cleaned data file to get column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NBB_nr</th>\n",
       "      <th>Year_Sentence_nr</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Muscular_Weakness</th>\n",
       "      <th>Spasticity</th>\n",
       "      <th>Hyperreflexia_and_oth_reflexes</th>\n",
       "      <th>Frontal_release_signs</th>\n",
       "      <th>Fasciculations</th>\n",
       "      <th>Positive_sensory_symptoms</th>\n",
       "      <th>Negative_sensory_symptoms</th>\n",
       "      <th>...</th>\n",
       "      <th>Orthostatic_hypotension</th>\n",
       "      <th>Headache_migraine</th>\n",
       "      <th>Fatique</th>\n",
       "      <th>Declined_deteriorated_health</th>\n",
       "      <th>Cachexia</th>\n",
       "      <th>Weight_loss</th>\n",
       "      <th>Reduces_oral_intake</th>\n",
       "      <th>Help_in_ADL</th>\n",
       "      <th>Day_care</th>\n",
       "      <th>Admission_to_nursing_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBB 1990-048</td>\n",
       "      <td>Past_sentence_0</td>\n",
       "      <td>Past: The patient was known to have atrial fib...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBB 1990-048</td>\n",
       "      <td>Past_sentence_1</td>\n",
       "      <td>The patient was known to have hypertension and...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBB 1990-048</td>\n",
       "      <td>1979_sentence_0</td>\n",
       "      <td>1979: She got a total hip</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBB 1990-048</td>\n",
       "      <td>1979_sentence_1</td>\n",
       "      <td>At age 76 the first demential symptomes appeared</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBB 1990-048</td>\n",
       "      <td>1979_sentence_2</td>\n",
       "      <td>After the death of her husband homesituation w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19049</th>\n",
       "      <td>NBB 2018-114</td>\n",
       "      <td>2018_sentence_25</td>\n",
       "      <td>The patient himself did not recognize himself ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19050</th>\n",
       "      <td>NBB 2018-114</td>\n",
       "      <td>2018_sentence_26</td>\n",
       "      <td>In July and August he suffered from deliria po...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19051</th>\n",
       "      <td>NBB 2018-114</td>\n",
       "      <td>2018_sentence_27</td>\n",
       "      <td>In August the GP reported that it was impossib...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19052</th>\n",
       "      <td>NBB 2018-114</td>\n",
       "      <td>2018_sentence_28</td>\n",
       "      <td>This was a reason why a hospice turned down an...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19053</th>\n",
       "      <td>NBB 2018-114</td>\n",
       "      <td>2018_sentence_29</td>\n",
       "      <td>In September the patient was euthanized and de...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18917 rows × 93 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             NBB_nr  Year_Sentence_nr  \\\n",
       "0      NBB 1990-048   Past_sentence_0   \n",
       "1      NBB 1990-048   Past_sentence_1   \n",
       "2      NBB 1990-048   1979_sentence_0   \n",
       "3      NBB 1990-048   1979_sentence_1   \n",
       "4      NBB 1990-048   1979_sentence_2   \n",
       "...             ...               ...   \n",
       "19049  NBB 2018-114  2018_sentence_25   \n",
       "19050  NBB 2018-114  2018_sentence_26   \n",
       "19051  NBB 2018-114  2018_sentence_27   \n",
       "19052  NBB 2018-114  2018_sentence_28   \n",
       "19053  NBB 2018-114  2018_sentence_29   \n",
       "\n",
       "                                                Sentence  Muscular_Weakness  \\\n",
       "0      Past: The patient was known to have atrial fib...                  0   \n",
       "1      The patient was known to have hypertension and...                  0   \n",
       "2                              1979: She got a total hip                  0   \n",
       "3       At age 76 the first demential symptomes appeared                  0   \n",
       "4      After the death of her husband homesituation w...                  0   \n",
       "...                                                  ...                ...   \n",
       "19049  The patient himself did not recognize himself ...                  0   \n",
       "19050  In July and August he suffered from deliria po...                  0   \n",
       "19051  In August the GP reported that it was impossib...                  0   \n",
       "19052  This was a reason why a hospice turned down an...                  0   \n",
       "19053  In September the patient was euthanized and de...                  0   \n",
       "\n",
       "       Spasticity  Hyperreflexia_and_oth_reflexes  Frontal_release_signs  \\\n",
       "0               0                               0                      0   \n",
       "1               0                               0                      0   \n",
       "2               0                               0                      0   \n",
       "3               0                               0                      0   \n",
       "4               0                               0                      0   \n",
       "...           ...                             ...                    ...   \n",
       "19049           0                               0                      0   \n",
       "19050           0                               0                      0   \n",
       "19051           0                               0                      0   \n",
       "19052           0                               0                      0   \n",
       "19053           0                               0                      0   \n",
       "\n",
       "       Fasciculations  Positive_sensory_symptoms  Negative_sensory_symptoms  \\\n",
       "0                   0                          0                          0   \n",
       "1                   0                          0                          0   \n",
       "2                   0                          0                          0   \n",
       "3                   0                          0                          0   \n",
       "4                   0                          0                          0   \n",
       "...               ...                        ...                        ...   \n",
       "19049               0                          0                          0   \n",
       "19050               0                          0                          0   \n",
       "19051               0                          0                          0   \n",
       "19052               0                          0                          0   \n",
       "19053               0                          0                          0   \n",
       "\n",
       "       ...  Orthostatic_hypotension  Headache_migraine  Fatique  \\\n",
       "0      ...                        0                  0        0   \n",
       "1      ...                        0                  0        0   \n",
       "2      ...                        0                  0        0   \n",
       "3      ...                        0                  0        0   \n",
       "4      ...                        0                  0        0   \n",
       "...    ...                      ...                ...      ...   \n",
       "19049  ...                        0                  0        0   \n",
       "19050  ...                        0                  0        0   \n",
       "19051  ...                        0                  0        0   \n",
       "19052  ...                        0                  0        0   \n",
       "19053  ...                        0                  0        0   \n",
       "\n",
       "       Declined_deteriorated_health  Cachexia  Weight_loss  \\\n",
       "0                                 0         0            0   \n",
       "1                                 0         0            0   \n",
       "2                                 0         0            0   \n",
       "3                                 0         0            0   \n",
       "4                                 0         0            0   \n",
       "...                             ...       ...          ...   \n",
       "19049                             0         0            0   \n",
       "19050                             0         0            0   \n",
       "19051                             0         0            0   \n",
       "19052                             0         0            0   \n",
       "19053                             0         0            0   \n",
       "\n",
       "       Reduces_oral_intake  Help_in_ADL  Day_care  Admission_to_nursing_home  \n",
       "0                        0            0         0                          0  \n",
       "1                        0            0         0                          0  \n",
       "2                        0            0         0                          0  \n",
       "3                        0            0         0                          0  \n",
       "4                        0            0         0                          0  \n",
       "...                    ...          ...       ...                        ...  \n",
       "19049                    0            0         0                          0  \n",
       "19050                    0            0         0                          0  \n",
       "19051                    0            0         0                          0  \n",
       "19052                    0            0         0                          0  \n",
       "19053                    0            0         0                          0  \n",
       "\n",
       "[18917 rows x 93 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cleaned_train = pd.read_excel(path_to_cleaned_training_data_xlsx, engine='openpyxl', index_col=[0])\n",
    "with open(path_to_cleaned_training_data_pkl,\"rb\") as file:\n",
    "    cleaned_train = pickle.load(file)\n",
    "display(cleaned_train)\n",
    "\n",
    "non_attribute_columns = ['NBB_nr','Year_Sentence_nr','Sentence']\n",
    "attributes = [col for col in cleaned_train.columns if col not in non_attribute_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna has an objective function which returns a value. Optuna then tries to maximize that value, where it tries new parameter combinations each trial. In our case the value we want to optimize is performance of all 90 attributes, averaged over the 5 folds. This is the average 5 fold micro F1 score. Creating the model and optimization itself is quite straightforward. For analysis purposes however, we also want to save other performance metrics for all trials into a csv file. We do not save the intermediate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    counter = 1\n",
    "    dataframe_list = []\n",
    "    \n",
    "    ## set up split\n",
    "    X = trainval[['text']].to_numpy() \n",
    "    Y = pd.DataFrame(trainval['labels'])\n",
    "    Y = pd.DataFrame(Y['labels'].to_list())\n",
    "    Y = Y.to_numpy() \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True,random_state=0)\n",
    "    \n",
    "    ## Split the train&val into multiple train and val sets.\n",
    "    for train_index, val_index in mskf.split(X, Y):\n",
    "        x_train, x_val = X[train_index], X[val_index] \n",
    "        y_train, y_val = Y[train_index], Y[val_index]\n",
    "        train = pd.DataFrame(x_train,columns=['text']) \n",
    "        val = pd.DataFrame(x_val,columns=['text'])\n",
    "        train['labels'] = y_train.tolist()\n",
    "        val['labels'] = y_val.tolist()\n",
    "        \n",
    "        print(\"Trial nr:\",trial.number,\"Fold number:\",counter,\"\\nTRAIN row numbers:\", train_index, \"\\nVAL row numbers:\", val_index)\n",
    "        \n",
    "        print(f\"nr of sentences in train: {len(x_train)}, or {len(x_train)/(len(x_train)+len(x_val)):.2f}%\\n\" \\\n",
    "              f\"nr of sentences in val: {len(x_val)}, or {len(x_val)/(len(x_train)+len(x_val)):.2f}% \\n\")\n",
    "        \n",
    "        ## to optimize\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-4)\n",
    "#         th = trial.suggest_float(\"th\", 0.4, 0.7)\n",
    "        ep = trial.suggest_int(\"ep\", 1, 2)#20,35\n",
    "        model_args_bert = { \"do_lower_case\": True, # for uncased models\n",
    "               \"fp16\": True,#speeds up, but risk under/overflow\n",
    "               \"learning_rate\": lr, # candidate for optimalisation\n",
    "               \"manual_seed\": 2,\n",
    "               \"max_seq_length\": 300, #Chosen such that most samples are not truncated. Increasing the sequence length significantly affects the memory consumption of the model, so it s usually best to keep it as short as possible (ideally without truncating the input sequences).\n",
    "               \"num_train_epochs\": ep, # option for optimalisation\n",
    "              #\"optimizer\": \"Adafactor\", # option for optimalisation\n",
    "               \"output_dir\": model_save_path + '/Optuna',\n",
    "               \"overwrite_output_dir\": True,\n",
    "               \"reprocess_input_data\" : True, #default true, input data will be reprocessed even if a cached file of the input data exists.\n",
    "               \"save_eval_checkpoints\":False,\n",
    "               \"save_model_every_epoch\":False,\n",
    "               \"save_optimizer_and_scheduler\":False,\n",
    "               \"save_steps\": -1,\n",
    "               \"silent\":False,\n",
    "              #\"scheduler\": \"linear_schedule_with_warmup\",  # option for optimalisation\n",
    "              #\"sliding_window\": True # not supported, but advised? # option for optimalisation\n",
    "               \"train_batch_size\": 16,  \n",
    "               \"use_multiprocessing\": True, #speeds up,may be unstable, has some issues reported with t5\n",
    "               \"wandb_project\": None,#\"pubmed_wandnm\",\n",
    "                \"wandb_kwargs\": {\"mode\":\"disabled\"},\n",
    "               \"threshold\":0.6}\n",
    "        model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "                                              \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", ## \"modelname from huggingface\"\n",
    "                                              args=model_args_bert,\n",
    "                                              use_cuda=False,#True,\n",
    "                                              num_labels=90)  \n",
    "        model.train_model(train)\n",
    "        \n",
    "        ## y_val: the true labels in numpy array form\n",
    "        y_val = val[\"labels\"]\n",
    "        y_val = pd.DataFrame(y_val) \n",
    "        y_val = pd.DataFrame(y_val['labels'].to_list())\n",
    "        y_val = y_val.to_numpy()\n",
    "        \n",
    "        ## the predicted labels in numpy array form\n",
    "        sentences = val[\"text\"].values\n",
    "        sentences = [str(i) for i in list(sentences)]\n",
    "        predictions, raw_outputs = model.predict(list(sentences))\n",
    "        y_val_predicted_labels_pubmedbert = np.array(predictions)\n",
    "         \n",
    "        all_metrics = classification_report(y_val, y_val_predicted_labels_pubmedbert,\n",
    "                                            target_names=attributes,digits=3,output_dict=True)   \n",
    "        all_metrics_df = pd.DataFrame(all_metrics).transpose()\n",
    "        all_metrics_df['Trial'] = trial.number\n",
    "        all_metrics_df['Fold'] = counter\n",
    "        dataframe_list.append(all_metrics_df)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    performance_all_folds = pd.concat(dataframe_list)\n",
    "    performance_all_folds = performance_all_folds.reset_index()\n",
    "    performance_all_folds = performance_all_folds.rename(columns={\"index\": \"Attribute\"})\n",
    "    average_performance = performance_all_folds.drop('Fold',axis=1)\n",
    "    average_performance = average_performance.groupby('Attribute').mean()\n",
    "    display(performance_all_folds)\n",
    "    micro_f1_score = average_performance.loc['micro avg']['f1-score'] \n",
    "    \n",
    "    ## add to csv file containing all trials for all folds\n",
    "    with open('{}/{}.csv'.format(model_save_path,study_name),'a') as f:\n",
    "        performance_all_folds.to_csv(f, header = False,index=False)\n",
    "    print(micro_f1_score)\n",
    "    print('-----------------------')\n",
    "    \n",
    "    \n",
    "    return micro_f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-27 12:47:14,478]\u001b[0m A new study created in RDB with name: test\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new study created in RDB with name: test\n",
      "A new study created in RDB with name: test\n",
      "A new study created in RDB with name: test\n",
      "A new study created in RDB with name: test\n",
      "creating csv file /home/jupyter-n.mekkes@gmail.com-f6d87/ext_n_mekkes_gmail_com/clinical_history/nlp_models/PubMedBERT/test/test.csv to save model performance\n"
     ]
    }
   ],
   "source": [
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "study = optuna.create_study(study_name=study_name,direction='maximize',load_if_exists=True, storage=storage_name)\n",
    "f = model_save_path + '/' + study_name + '.csv'\n",
    "file_exists = os.path.isfile(f)\n",
    "if not file_exists:\n",
    "    print('creating csv file {} to save model performance'.format(f))\n",
    "    with open(f, 'w') as f:\n",
    "        writer = csv.writer(f,delimiter=',', lineterminator='\\n')\n",
    "        writer.writerow(['Attribute','Precision','Recall','F1','Support','Trial','Fold'])\n",
    "        f.close()\n",
    "        \n",
    "study = optuna.load_study(study_name=study_name, storage=storage_name)#,\"maximize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimize Optuna\n",
    "IMPORTANT: if you have already finished an optuna trial and are pleased with the results, and do not want additional trials, do not run this block. <br>\n",
    "Typically I run for 30 Trials, we do not see improvement after 30 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial nr: 0 Fold number: 1 \n",
      "TRAIN row numbers: [    0     2     3 ... 15127 15128 15131] \n",
      "VAL row numbers: [    1    12    20 ... 15129 15130 15132]\n",
      "nr of sentences in train: 12106, or 0.80%\n",
      "nr of sentences in val: 3027, or 0.20% \n",
      "\n",
      "Trial nr: 1 Fold number: 1 \n",
      "TRAIN row numbers: [    0     2     3 ... 15127 15128 15131] \n",
      "VAL row numbers: [    1    12    20 ... 15129 15130 15132]\n",
      "nr of sentences in train: 12106, or 0.80%\n",
      "nr of sentences in val: 3027, or 0.20% \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ff392ec1e946d18e21d4aee77ff80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMultiLabelSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a1a2e6a32043f8befb41fc5d72be08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/225k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertForMultiLabelSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultiLabelSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultiLabelSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6d4ba543b64aba986390e670c68411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e6a639c618457ca3c74b0bc582fa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ccf2e1d68b48fd9ffdd84ffa99970f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-d31546decf4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConvergenceWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## additional, store using joblib:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         )\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    115\u001b[0m                             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                             \u001b[0mtime_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                             \u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                         )\n\u001b[1;32m    119\u001b[0m                     )\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "    study.optimize(objective, n_trials=2,n_jobs=2)\n",
    "\n",
    "## additional, store using joblib:\n",
    "savename = '{}/{}.pkl'.format(model_save_path,study_name)\n",
    "joblib.dump(study, savename)\n",
    "\n",
    "print('finished optimization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick overview of Optuna results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best trial:\n",
    "print(f\"The best trial is : \\n{study.best_trial} \\n\")\n",
    "\n",
    "# Getting the best score:\n",
    "print(f\"The highest f1 value is : \\n{study.best_value}\\n\")\n",
    "\n",
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures to illustrate optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 1: Hyperparameter importance\n",
    "Which hyperparameter has the most influence on model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pai = plot_param_importances(study)\n",
    "pai.update_layout(\n",
    "    title='{} Hyperparameter importance'.format(used_model),\n",
    "    xaxis_title='Importance for F1-score',\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pai.update_xaxes(range=[0,1])\n",
    "pai.write_image(save_path_figures + \"/{}_{}_parameter_importance.png\".format(used_model,study_name))\n",
    "pai.write_image(save_path_figures + \"/{}_{}_parameter_importance.pdf\".format(used_model,study_name))\n",
    "pai.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 2: optuna visualisation plot optimization\n",
    "Improvement over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oph = plot_optimization_history(study,target_name='F1-score')\n",
    "oph.update_layout(\n",
    "    title='{} Optimization history'.format(used_model),\n",
    "    xaxis_title='Number of trials',\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=18,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "oph.update_yaxes(range=[0,1])\n",
    "oph.write_image(save_path_figures + \"/{}_{}_optimization_history.png\".format(used_model,study_name))\n",
    "oph.write_image(save_path_figures + \"/{}_{}_optimization_history.pdf\".format(used_model,study_name))\n",
    "oph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3: optuna visualisation parallel coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = plot_parallel_coordinate(study,target_name='F1-score')\n",
    "\n",
    "pac.update_layout(\n",
    "    title='{} Parallel Coordinate Plot'.format(used_model),\n",
    "    yaxis_title=\"F1-score\",\n",
    "    height=400,\n",
    "    margin=dict(\n",
    "        pad=40\n",
    "    ),\n",
    "    autosize=False,\n",
    "    font=dict(\n",
    "        family=\"Arial, monospace\",\n",
    "        size=16,\n",
    "        color=\"Black\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pac.write_image(save_path_figures + \"/{}_{}_parallel_coordinate.png\".format(used_model,study_name))\n",
    "pac.write_image(save_path_figures + \"/{}_{}_parallel_coordinate.pdf\".format(used_model,study_name))\n",
    "pac.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 4: All trials; plot improvement over trials (f1 and precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_number,best_trial,f1_precision_long,micro_F1,micro_Precision = analysis_performance(model_save_path,study_name,used_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(f1_precision_long,\n",
    "            save_path_figures,\n",
    "            used_model,\n",
    "            study_name,\n",
    "            best_trial_number,\n",
    "            metric='F1',\n",
    "            pal='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trials(f1_precision_long,\n",
    "            save_path_figures,\n",
    "            used_model,\n",
    "            study_name,\n",
    "            best_trial_number,\n",
    "            metric='Precision',\n",
    "            pal='Oranges')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute scatter\n",
    "We have selected the best trial. How do the different attributes perform under this trial? Some might be bad. We plot the precision against the F1 score for all attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0], sheet_name='90 parameters')\n",
    "display(attribute_grouping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_names = {}\n",
    "for attr, real_name in zip(attribute_grouping.index, attribute_grouping[\"Attribute\"]):\n",
    "    if not isinstance(real_name, float):\n",
    "        correct_names[real_name] = attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = best_trial.rename(index=correct_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select some attributes that we want to plot with their text label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_attributes = ['Unspecified_disturbed_gait_patt',\n",
    "'Loss_of_sympathy_empathy',\n",
    "'Fasciculations',\n",
    "# 'Psychiatric_admissions',\n",
    "'Changed_moods_emotions',\n",
    "'Bradyphrenia',\n",
    "'Head_turning_sign',\n",
    "# 'Communication_problems',\n",
    "# 'Decreased_motor_skills',\n",
    "'Language_impairment',\n",
    "# 'Positive_sensory_symptoms',\n",
    "'Facade_behavior',\n",
    "# 'Impaired_comprehension',\n",
    "'Changed_behavior_personality',\n",
    "'Frontal_release_signs',\n",
    "'Vivid_dreaming',\n",
    "'Loss_of_sympathy_empathy'\n",
    "]\n",
    "\n",
    "fancy_bad_attributes = [correct_names.get(item,item)  for item in bad_attributes]\n",
    "print(fancy_bad_attributes)\n",
    "\n",
    "good_attributes = [\n",
    "# 'Parkinsonism',\n",
    "'Memory_impairment',\n",
    "'Mobility_problems',\n",
    "'Fatigue',\n",
    "'Depressed_mood'\n",
    "# 'Psychosis',\n",
    "'Agitation',\n",
    "# 'Fatigue',\n",
    "]\n",
    "\n",
    "fancy_good_attributes = [correct_names.get(item,item)  for item in good_attributes]\n",
    "print(fancy_good_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot for optuna best trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = best_trial.drop([\"micro avg\", \"macro avg\", \"weighted avg\",\"samples avg\"],errors='ignore')\n",
    "scatter_plot(best_trial, \n",
    "             used_model,\n",
    "             study_name,\n",
    "             fancy_good_attributes,\n",
    "             fancy_bad_attributes,\n",
    "             metrics='Precision_F1',\n",
    "             printf1=round(micro_F1,5),\n",
    "             printprec=round(micro_Precision,5),\n",
    "             trialname=best_trial_number,\n",
    "             wheretosave=save_path_figures,\n",
    "             val_or_test='Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on train&val data with best parameters, evaluate on hold-out test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train on the combined training and validation data. we test on the kept apart testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_test_pkl,\"rb\") as file:\n",
    "    test = pickle.load(file)\n",
    "\n",
    "# test[\"labels\"] = [eval(row[\"labels\"]) for index, row in test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args_PubMedBERT = { \"do_lower_case\": True, # for uncased models\n",
    "       \"fp16\": True,#speeds up, but risk under/overflow\n",
    "       \"learning_rate\": study.trials[best_trial_number].params['lr'], # candidate for optimalisation\n",
    "       \"manual_seed\": 2,\n",
    "       \"max_seq_length\": 300, #Chosen such that most samples are not truncated. Increasing the sequence length significantly affects the memory consumption of the model, so it s usually best to keep it as short as possible (ideally without truncating the input sequences).\n",
    "       \"num_train_epochs\": study.trials[best_trial_number].params['ep'], # option for optimalisation\n",
    "      #\"optimizer\": \"Adafactor\", # option for optimalisation\n",
    "       \"output_dir\": path_final,\n",
    "       \"overwrite_output_dir\": True,\n",
    "       \"reprocess_input_data\" : True, #default true, input data will be reprocessed even if a cached file of the input data exists.\n",
    "       \"save_eval_checkpoints\":False,\n",
    "       \"save_model_every_epoch\":False,\n",
    "       \"save_optimizer_and_scheduler\":False,\n",
    "       \"save_steps\": -1,\n",
    "       \"silent\":False,\n",
    "      #\"scheduler\": \"linear_schedule_with_warmup\",  # option for optimalisation\n",
    "      #\"sliding_window\": True # not supported, but advised? # option for optimalisation\n",
    "       \"train_batch_size\": 16,  \n",
    "       \"use_multiprocessing\": True, #speeds up,may be unstable, has some issues reported with t5\n",
    "       \"wandb_project\": None,\n",
    "        \"wandb_kwargs\": {\"mode\":\"disabled\"},\n",
    "       \"threshold\":0.6\n",
    " }\n",
    "\n",
    "## when training for the first time\n",
    "model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "                                      \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\", ## \"modelname from huggingface\"\n",
    "                                      args=model_args_PubMedBERT,\n",
    "                                      use_cuda=True,\n",
    "                                      num_labels=90)\n",
    "\n",
    "model.train_model(trainval)\n",
    "\n",
    "# ## when loading already trained model\n",
    "# model = MultiLabelClassificationModel('bert', ## \"bert\" or \"t5\"\n",
    "#                                       path_final, ## \"modelname from huggingface\"\n",
    "#                                       args=model_args_PubMedBERT,\n",
    "#                                       use_cuda=True,#True,\n",
    "#                                       num_labels=90) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the truths\n",
    "y_test = test['labels']\n",
    "y_test = pd.DataFrame(y_test) \n",
    "y_test = pd.DataFrame(y_test['labels'].to_list())\n",
    "y_test = y_test.to_numpy()\n",
    "\n",
    "\n",
    "# Create a list with the text to predict (parsed sentences)\n",
    "sentences = test['text'].values#val[\"text\"].values\n",
    "sentences = [str(i) for i in list(sentences)]\n",
    "predictions, raw_outputs = model.predict(list(sentences))\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "test_report = classification_report(y_test, predictions, target_names=attributes,\n",
    "                                       digits=3,output_dict=True)\n",
    "test_report_df = pd.DataFrame(test_report).transpose()\n",
    "test_report_df.columns=['Precision','Recall','F1','Support']\n",
    "F1 = test_report_df.loc['micro avg']['Precision'] #!4\n",
    "Precision = test_report_df.loc['micro avg']['F1']\n",
    "print('F1-score micro: ', F1)\n",
    "print('Precision micro: ', Precision)\n",
    "\n",
    "display(test_report_df)\n",
    "\n",
    "## save the performance\n",
    "test_report_df.to_csv(model_save_path +'/{}_{}_test_performance.csv'.format(used_model,study_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the sentences with their truth label and prediction label, for investigation\n",
    "truths = pd.DataFrame(y_test,columns=attributes).add_suffix('_Truth')\n",
    "predictions_df = pd.DataFrame(predictions,columns=attributes).add_suffix('_Prediction')\n",
    "sentences = pd.concat([truths,predictions_df], axis=1)\n",
    "sentences = sentences.reindex(sorted(sentences.columns), axis=1)\n",
    "sentences.insert(loc=0, column='Sentence', value=test['text'])\n",
    "\n",
    "display(sentences)\n",
    "# sentences.to_csv(model_save_path +'/{}_{}_test_sentences.csv'.format(used_model,study_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report_df['pass_fail'] = np.where(((test_report_df.F1 >= 0.8) | (test_report_df.Precision >= 0.8) ),'pass', 'fail')\n",
    "test_report_df = test_report_df.drop([\"micro avg\", \"macro avg\", \"weighted avg\",\"samples avg\"],errors='ignore')\n",
    "display(test_report_df)\n",
    "scatter_plot(test_report_df, \n",
    "             used_model,\n",
    "             study_name,\n",
    "             fancy_good_attributes,\n",
    "             fancy_bad_attributes,\n",
    "             metrics='Precision_F1',\n",
    "             printf1=round(F1,5),\n",
    "             printprec=round(Precision,5),\n",
    "             trialname='',\n",
    "             wheretosave=save_path_figures,\n",
    "             val_or_test='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
