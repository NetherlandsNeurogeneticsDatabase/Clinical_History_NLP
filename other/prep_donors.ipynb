{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Neurogenomics Database: Dotplot of entire dataset predictions\n",
    "Author: Nienke Mekkes <br>\n",
    "Date: 11-10-2022. <br>\n",
    "Correspond: n.j.mekkes@umcg.nl <br>\n",
    "\n",
    "## Script: Dotplot of entire dataset predictions\n",
    "Builds Dot Plots for each diagnosis category. <br>\n",
    "Why: to give an overview of what symptoms are frequently observed in different diagnosis groups\n",
    "\n",
    "### Input files:\n",
    "- prediction file (donors as row names, observations as columns)\n",
    "- General information: to assign metadata to donors (e.g. diagnosis, age)\n",
    "- Optional: attribute metadata to cluster observations\n",
    "- Optional: metadata to highlight expected findings in the plot\n",
    "\n",
    "- also needs scattermap.py, code to create the plot\n",
    "- also needs helper_functions, which contains code to run permutation test and how to select donors\n",
    "\n",
    "\n",
    "### Output:\n",
    "- dotplot, file with p values for permutation test\n",
    "\n",
    "\n",
    "\n",
    "#### Minimal requirements\n",
    "- to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this script works with a clinical trajectory dictionary pickle. this pickle can be a rules of thumb or a original pickle, and was generated by the script proces_predictions. This processing script removed short sentences etc. and the attributes that performed poorly. This processing script did not remove any donors. Donors that you wish to be excluded can be excluded in two ways: <br>\n",
    "1. in this script, manually. for example remove donors younger than 21. or donors with the NAD diagnosis, or reassign diagnosis (e.g. a SSA, CON donor NBB xxx needs to become HIV).\n",
    "2. with an input file, for example the general information that contains minimally one column with donorids, and one column that mentions which donors should have a changed diagnosis or should be excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_predictions = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/final_predictions/ALL_clinical_trajectories_dictionary_2023-07-11.pkl\"\n",
    "path_to_predictions = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/final_predictions/ALL_clinical_trajectories_dictionary_rules_of_thumb_visit_2023-08-14.pkl\"\n",
    "# path_to_attribute_grouping = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/sup3.xlsx\" ## for rules of thumb\n",
    "general_information = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/input_data/General_information_11-08-2023.xlsx\"\n",
    "path_clinical_diagnosis = '/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_diagnosis/output/selected_diagnoses_overview.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import matplotlib\n",
    "import numpy as np; np.random.seed(0)\n",
    "from matplotlib import pyplot as plt \n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import scattermap\n",
    "from scattermap import scattermap\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import statsmodels\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "import scipy\n",
    "from helper_functions import permutation_of_individual_test, table_selector\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_predictions,\"rb\") as file:\n",
    "    predictions_pickle = pickle.load(file)\n",
    "\n",
    "d = []\n",
    "for i,j in zip(predictions_pickle,predictions_pickle.values()):\n",
    "    k = pd.DataFrame.from_dict(j,orient=\"index\")\n",
    "    k[\"DonorID\"] = i\n",
    "    k['Age'] = k.index\n",
    "    d.append(k)\n",
    "\n",
    "predictions_df =pd.concat(d, ignore_index=True)\n",
    "display(predictions_df)\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n",
    "print(predictions_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exclude/change donors for the paper, using general info\n",
    "- read in the general information\n",
    "- make a list of donors to remove\n",
    "- remove donors from our predictions\n",
    "- change column neuropathological diagnosis to the neuropathological diagnosis from the general information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_information_df = pd.read_excel(general_information, engine='openpyxl', sheet_name=\"Sheet1\")\n",
    "donors_to_remove = list(general_information_df[general_information_df['paper diagnosis']=='exclude'].DonorID)\n",
    "predictions_df = predictions_df[~predictions_df['DonorID'].isin(donors_to_remove)]\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n",
    "print(len(donors_to_remove))\n",
    "predictions_df['neuropathological_diagnosis'] = predictions_df['DonorID'].map(general_information_df.set_index('DonorID')['paper diagnosis'])\n",
    "display(predictions_df.head())\n",
    "print(sorted(predictions_df['neuropathological_diagnosis'].unique()))\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['Age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_attribute_columns = ['DonorID','Year','age_at_death','sex',\n",
    "                        'neuropathological_diagnosis','Age'] #'birthyear',,'death_year','year_before_death','sex',\n",
    "attributes = [col for col in predictions_df.columns if col not in non_attribute_columns]\n",
    "# display(attributes)\n",
    "print(f\"there are {predictions_df.shape[0]} rows and {len(attributes)} attributes\")\n",
    "print(f\"there are {len(list(predictions_df['DonorID'].unique()))} unique donor IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### adding in clinical diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_df = pd.read_excel(path_clinical_diagnosis, engine='openpyxl')\n",
    "# cd_df\n",
    "# predictions_df['perfect_diagnosis'] = predictions_df['DonorID'].map(cd_df.set_index('DonorID')['perfect_diagnosis'])\n",
    "# predictions_df['medium_diagnosis'] = predictions_df['DonorID'].map(cd_df.set_index('DonorID')['medium_diagnosis'])\n",
    "# predictions_df['wrong_diagnosis'] = predictions_df['DonorID'].map(cd_df.set_index('DonorID')['wrong_diagnosis'])\n",
    "# def get_diagnosis_info(row):\n",
    "#     if row['perfect_diagnosis'] == 1:\n",
    "#         return 'perfect'\n",
    "#     elif row['medium_diagnosis'] == 1:\n",
    "#         return 'medium'\n",
    "#     elif row['wrong_diagnosis'] == 1:\n",
    "#         return 'wrong'\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# predictions_df['diagnosis_info'] = predictions_df.apply(get_diagnosis_info, axis=1)\n",
    "# predictions_df = predictions_df.drop(columns=['perfect_diagnosis','medium_diagnosis','wrong_diagnosis'])\n",
    "# predictions_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_dict_paper = {\n",
    "                'CON': 'CON',\n",
    "                'AD': 'AD',\n",
    "                'PD': 'PD',\n",
    "                'PDD':'PDD',\n",
    "                'DLB':'DLB',\n",
    "                'VD' : 'VD',\n",
    "\n",
    "                'FTD,FTD-TDP':'FTD','FTD,FTD-TDP-A,PROG':'FTD','FTD,FTD-TDP-B,C9ORF72':'FTD','FTD,FTD-TDP-C':'FTD', \n",
    "                'FTD,FTD-TAU,TAU':'FTD',\n",
    "                'FTD,FTD-FUS':'FTD',\n",
    "                'FTD,FTD-TDP,MND':'FTD',\n",
    "                # 'FTD,FTD-UPS':'FTD',               \n",
    "                'FTD,PID':'FTD',\n",
    "                # 'FTD':'FTD', \n",
    "    'FTD_undefined':'FTD',\n",
    "    'FTD,FTD-TDP_undefined':'FTD',\n",
    "\n",
    "                'MND,ALS':'MND',\n",
    "                'MND_other':'MND',\n",
    "\n",
    "                'PSP' : 'PSP',\n",
    "\n",
    "                'ATAXIA,SCA':'ATAXIA',\n",
    "                'ATAXIA,ADCA':'ATAXIA',\n",
    "                'ATAXIA,FA':'ATAXIA',\n",
    "                'ATAXIA,FXTAS':'ATAXIA',\n",
    "\n",
    "                'MS,MS-PP':'MS',\n",
    "                'MS,MS-SP':'MS',\n",
    "                # 'MS,MS-UN':'MS',\n",
    "                'MS,MS-RR':'MS',\n",
    "                'MS_undefined':'MS',\n",
    "\n",
    "                'MSA' : 'MSA',\n",
    "                'PSYCH,MDD':'MDD',\n",
    "                'PSYCH,BP':'BP',\n",
    "                'PSYCH,SCZ':'SCZ'\n",
    "                                }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = predictions_df[predictions_df.Age >= 0]\n",
    "display(data['DonorID'].nunique())\n",
    "data = predictions_df.copy()\n",
    "display(data['DonorID'].nunique())\n",
    "data['file_year'] = data['DonorID'].str.extract(r'NBB (\\d{4})-\\d{3}', expand=False)\n",
    "data['file_year'] = pd.to_numeric(data['file_year'])\n",
    "data = data[data['file_year'] >= 1997]\n",
    "display(data['DonorID'].nunique())\n",
    "unique_diagnoses = data[['DonorID', 'neuropathological_diagnosis']].drop_duplicates()\n",
    "display(unique_diagnoses['neuropathological_diagnosis'].value_counts().head(20))\n",
    "# display(merged_df.head())\n",
    "## how many observations has each donor?\n",
    "data2 = data.copy()\n",
    "# display(data2['Age'].drop_duplicates().sort_values())\n",
    "# display(data.groupby('DonorID')['Age'].nunique())\n",
    "\n",
    "## df showing number of observations\n",
    "data2 = data2.drop(columns=['age_at_death','sex','Age','file_year','Year'])#,'diagnosis_info'])\n",
    "data2 = data2.groupby(['DonorID','neuropathological_diagnosis']).sum()\n",
    "data2 = pd.DataFrame(data2.sum(axis=1),columns=['count'])\n",
    "data2 = data2.reset_index()  \n",
    "data2 = data2.set_index('DonorID')\n",
    "data2['uniqueage'] = data.groupby('DonorID')['Age'].nunique()\n",
    "display(data2)\n",
    "\n",
    "# ## con are the exception, they are allowed to have little data\n",
    "data3 = data2[data2['neuropathological_diagnosis'] != 'CON']\n",
    "donors_not_enough_data = data3.index[data3['count'] < n].tolist()\n",
    "# # donors_not_enough_data = data3.index[(data3['count'] < 5) | (data3['uniqueage'] < 3)].tolist()\n",
    "\n",
    "\n",
    "print(donors_not_enough_data)\n",
    "# print(len(donors_not_enough_data))\n",
    "data = data[~data['DonorID'].isin(donors_not_enough_data)]\n",
    "data = data.reset_index(drop=True)\n",
    "display(data['DonorID'].nunique())\n",
    "data['neuropathological_diagnosis'] = data['neuropathological_diagnosis'].replace('PDD', 'PD')\n",
    "data['simplified_diagnosis'] = data['neuropathological_diagnosis'].map(table1_dict_paper)\n",
    "data['simplified_diagnosis'] = data['neuropathological_diagnosis'].apply(lambda x: 'AD,DLB' if x == 'AD,DLB' else table1_dict_paper.get(x, None))\n",
    "\n",
    "other_dems = ['CBD','AD,DLB','AD,CA','AD,ENCEPHA,VE','PD,AD', #,'ILBD','AD,ILBD','ENCEPHA,VE'\n",
    "              'DLB,SICC','DEM,SICC','DEM,SICC,AGD','DEM,ENCEPHA,VE']\n",
    "other_psych = ['PSYCH,PTSD','PSYCH,ASD','PSYCH,OCD']\n",
    "\n",
    "def update_psych(row):\n",
    "    if row['neuropathological_diagnosis'] in other_psych:\n",
    "        return 'other_psych'\n",
    "    return row['simplified_diagnosis']\n",
    "\n",
    "def update_dem(row):\n",
    "    if row['neuropathological_diagnosis'] in other_dems:\n",
    "        return 'other_dem'\n",
    "    return row['simplified_diagnosis']\n",
    "\n",
    "\n",
    "\n",
    "data['simplified_diagnosis'] = data.apply(update_psych, axis=1)\n",
    "data['simplified_diagnosis'] = data.apply(update_dem, axis=1)\n",
    "data['simplified_diagnosis'] = data['simplified_diagnosis'].apply(lambda x: 'Other' if x is None else x)\n",
    "display(data.head())\n",
    "display(data['Age'].drop_duplicates().sort_values())\n",
    "unique_diagnoses = data[['DonorID', 'simplified_diagnosis','neuropathological_diagnosis']].drop_duplicates()\n",
    "display(unique_diagnoses.tail(10))\n",
    "display(unique_diagnoses['simplified_diagnosis'].value_counts().head(60))\n",
    "display(data['DonorID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['simplified_diagnosis']=='Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for seurat we select certain donors based on neuropathological_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_diagnoses = data[['DonorID', 'neuropathological_diagnosis']].drop_duplicates()\n",
    "# display(unique_diagnoses)\n",
    "alldiag = list(unique_diagnoses['neuropathological_diagnosis'].unique())\n",
    "print(list(alldiag))\n",
    "len(unique_diagnoses[unique_diagnoses['neuropathological_diagnosis'] == 'VD,ILBD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data['neuropathological_diagnosis'].unique())\n",
    "\n",
    "seurat_diagnoses = [## main diagnoses \n",
    "                    'AD','DLB','VD','CON','PD','PSP','MSA','MND,ALS','MND_other',\n",
    "                    ## other dementias\n",
    "                    'CBD','AD,DLB','AD,CA','AD,ENCEPHA,VE','PD,AD', #,'ILBD','AD,ILBD','ENCEPHA,VE'\n",
    "                    'DLB,SICC','DEM,SICC','DEM,SICC,AGD','DEM,ENCEPHA,VE',\n",
    "                    ## ataxia subtypes\n",
    "                    'ATAXIA,SCA','ATAXIA,ADCA','ATAXIA,FXTAS','ATAXIA,FA',\n",
    "                    ### FTD subtypes\n",
    "                    'FTD_undefined','FTD,FTD-TAU,TAU','FTD,PID','FTD,FTD-FUS','FTD,FTD-TDP,MND',\n",
    "                    'FTD,FTD-TDP_undefined','FTD,FTD-TDP-A,PROG','FTD,FTD-TDP-B,C9ORF72','FTD,FTD-TDP-C',\n",
    "                    ## psych subtypes\n",
    "                    'PSYCH,MDD','PSYCH,BP','PSYCH,SCZ','PSYCH,PTSD','PSYCH,ASD','PSYCH,OCD',\n",
    "                    ## MS subtypes\n",
    "                    'MS,MS-SP','MS,MS-PP','MS_undefined','MS,MS-RR']\n",
    "\n",
    "not_used_seurat = list(set(alldiag) - set(seurat_diagnoses))\n",
    "not_used_seurat = data[data['neuropathological_diagnosis'].isin(not_used_seurat)]\n",
    "unique_diagnoses = not_used_seurat[['DonorID', 'simplified_diagnosis','neuropathological_diagnosis']].drop_duplicates()\n",
    "# display(unique_diagnoses['neuropathological_diagnosis'].value_counts())\n",
    "seurat = data[data['neuropathological_diagnosis'].isin(seurat_diagnoses)]\n",
    "seurat = seurat[seurat['DonorID'] != 'NBB 1999-072'] ## this donor is cursed :) \n",
    "unique_diagnoses = seurat[['DonorID', 'simplified_diagnosis','neuropathological_diagnosis']].drop_duplicates()\n",
    "display(unique_diagnoses['simplified_diagnosis'].value_counts())\n",
    "# display(unique_diagnoses['neuropathological_diagnosis'].value_counts())\n",
    "# display(unique_diagnoses.tail(20))\n",
    "# display(unique_diagnoses[unique_diagnoses['neuropathological_diagnosis']=='CBD'])\n",
    "display(seurat['DonorID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(unique_diagnoses['DonorID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note, now does not have clinical diagnosis! first do analysis, then load the clindiag in seurat itself\n",
    "seurat.to_csv('/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_analysis/data/seurat_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['DonorID']=='NBB 1999-072']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for the GRU-D model we only take the simplified donors, and delete the psychiatric cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_d = data.copy()\n",
    "def update_b(row):\n",
    "    if 'AD,DLB' in row['neuropathological_diagnosis']:\n",
    "        return 'AD,DLB'\n",
    "    return row['simplified_diagnosis']\n",
    "gru_d['simplified_diagnosis'] = gru_d.apply(update_b, axis=1)\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'Other']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'other_dem']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'other_psych']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'MDD']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'BP']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'SCZ']\n",
    "datalist = gru_d['DonorID'].unique()\n",
    "display(len(datalist))\n",
    "# unique_diagnoses = gru_d[['DonorID', 'simplified_diagnosis','neuropathological_diagnosis']].drop_duplicates()\n",
    "# display(unique_diagnoses.tail(10))\n",
    "# display(unique_diagnoses['simplified_diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_d = seurat.copy()\n",
    "def update_b(row):\n",
    "    if 'AD,DLB' in row['neuropathological_diagnosis']:\n",
    "        return 'AD,DLB'\n",
    "    return row['simplified_diagnosis']\n",
    "gru_d['simplified_diagnosis'] = gru_d.apply(update_b, axis=1)\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'Other']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'other_dem']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'other_psych']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'MDD']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'BP']\n",
    "gru_d = gru_d[gru_d['simplified_diagnosis'] != 'SCZ']\n",
    "seuratlist = gru_d['DonorID'].unique()\n",
    "display(len(seuratlist))\n",
    "# unique_diagnoses = gru_d[['DonorID', 'simplified_diagnosis','neuropathological_diagnosis']].drop_duplicates()\n",
    "# display(unique_diagnoses.tail(10))\n",
    "# display(unique_diagnoses['simplified_diagnosis'].value_counts())\n",
    "print(set(datalist) - set(seuratlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(unique_diagnoses['DonorID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### then we need to sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru_d.to_csv('/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_analysis/data/donors.csv', index=False)\n",
    "gru_d = gru_d[gru_d.Age >= 0]\n",
    "display(gru_d['DonorID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_d = gru_d.sort_values(['DonorID', 'Age'],\n",
    "              ascending = [True, True])\n",
    "display(gru_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_d.to_csv('/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_analysis/data/grud_clin_subset_input.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INPUT\n",
    "array of arrays. each array is for a single donor, consisting of shape time x attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = gru_d.copy()\n",
    "inp = inp.drop(['neuropathological_diagnosis','file_year','simplified_diagnosis','Year'],axis=1)#'diagnosis_info',\n",
    "inp['sex'] = inp['sex'].map({'F': 1, 'M': 0}).astype(int)\n",
    "inp['Age']  = inp['Age'].astype(int)\n",
    "inp['age_at_death']  = inp['age_at_death'].astype(int)\n",
    "def sum_except_donors(df):\n",
    "    return df.iloc[:, ].sum()\n",
    "\n",
    "inp = inp.sort_values(['DonorID', 'Age'],\n",
    "              ascending = [True, True])\n",
    "\n",
    "inp_with_nan = inp.copy()\n",
    "inp_with_nan = inp_with_nan.reset_index(drop=True)\n",
    "# final_row = inp_with_nan.groupby(['DonorID','sex']).sum().reset_index()\n",
    "# final_row['Age'] = 150\n",
    "# display(final_row)\n",
    "# inp_with_nan = inp_with_nan.drop(['Age'], axis=1)\n",
    "# inp_with_nan = pd.concat([inp_with_nan, final_row], ignore_index=True)\n",
    "display(inp_with_nan.head(5))\n",
    "final_input = inp_with_nan.set_index('DonorID')\n",
    "# display(final_input)\n",
    "final_input = final_input.groupby('DonorID').apply(pd.DataFrame.to_numpy).to_numpy()\n",
    "print(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_input.shape)\n",
    "print(final_input[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LABEL TASKNAME\n",
    "\n",
    "array in the shape samples X diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=30)\n",
    "lt = gru_d[['DonorID','simplified_diagnosis']].copy()\n",
    "# lt = lt[~lt['DonorID'].isin(weirds)]\n",
    "lt = lt.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "display(lt)\n",
    "\n",
    "donorcount = len(lt['DonorID'])\n",
    "print(donorcount)\n",
    "print(lt['simplified_diagnosis'].value_counts())\n",
    "print(lt.simplified_diagnosis.unique())\n",
    "\n",
    "one_hot = pd.get_dummies(lt.simplified_diagnosis)\n",
    "\n",
    "# Define the ordered list\n",
    "wanted = ['CON', 'AD', 'PD', 'VD', 'FTD','DLB','AD,DLB','ATAXIA', 'MND', 'PSP', 'MS','MSA'] #'AD,DLB' 'DLB,SICC',\n",
    "\n",
    "# Get the current columns of the dataframe\n",
    "current_cols = list(one_hot.columns)\n",
    "\n",
    "# Create a new list of columns in the order of the 'wanted' list\n",
    "new_cols = [col for col in wanted if col in current_cols]\n",
    "\n",
    "# Reorder the columns of the dataframe using the new list of columns\n",
    "one_hot = one_hot.reindex(columns=new_cols)\n",
    "display(one_hot)\n",
    "final_label_taskname = one_hot.to_numpy()\n",
    "display(final_label_taskname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MASKING\n",
    "Masking is a datatype of the same shape as input. it is used to indicate which data is present, and which data is absent. If a row of data would be [nan, nan, 0, 0, 3.14, 10], the masking row would be [0,0,1,1,1,1]. In our case, we have options:\n",
    "- every value larger than 1 to 1, every zero to zero. because a zero can mean that the symptom is present, it is just not written down that year?\n",
    "- every value to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "## simplest\n",
    "final_masking = copy.deepcopy(final_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "# for i in range(len(final_masking)):\n",
    "#     final_masking[i][final_masking[i] > 1] = 1\n",
    "    \n",
    "# option 2\n",
    "for k in range(len(final_masking)):\n",
    "    final_masking[k][final_masking[k] >= 0] = 1    \n",
    "\n",
    "print(final_masking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIMESTAMP\n",
    "Timestamp is another array of arrays. There is an array for every donor, that consists of the timepoints that are known for that donor. e.g. if donor1 has information from age 34, 61, and 62, then his timestamp would be [34,61,62]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_df = inp[['DonorID','Age']].copy()\n",
    "timestamp_df\n",
    "final_timestamp = timestamp_df.set_index('DonorID').groupby('DonorID').apply(pd.DataFrame.to_numpy).to_numpy()\n",
    "final_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLITTING BALANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function takes the created train and test data as input\n",
    "## it returns a measure of how similar train is to test\n",
    "## it also shows an overview of the number of cases per attribute\n",
    "## and a corrected version of this overview\n",
    "def split_vis(x_train,x_test,y_train, y_test,train_val_size,test_size):\n",
    "    \"\"\"\n",
    "    something\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    counts[\"train_counts\"] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "        y_train, order=1) for combination in row)\n",
    "    counts[\"test_counts\"] = Counter(str(combination) for row in get_combination_wise_output_matrix(\n",
    "        y_test, order=1) for combination in row)    \n",
    "\n",
    "    # view distributions\n",
    "    multi_split_dist = pd.DataFrame({\n",
    "        \"for_train_and_val\": counts[\"train_counts\"],\n",
    "        \"test\": counts[\"test_counts\"]\n",
    "    }).T.fillna(0)\n",
    "    multi_split_dist = multi_split_dist.reindex(natsorted(multi_split_dist.columns), axis=1)\n",
    "#     multi_split_dist.columns = labels\n",
    "    \n",
    "    for k in counts[\"test_counts\"].keys():\n",
    "        counts[\"test_counts\"][k] = int(counts[\"test_counts\"][k] * (train_val_size/test_size))\n",
    "        \n",
    "    # View size corrected distributions\n",
    "    multi_split_dist_corr = pd.DataFrame({\n",
    "        \"for_train_and_val\": counts[\"train_counts\"],\n",
    "        \"test\": counts[\"test_counts\"]\n",
    "    }).T.fillna(0)\n",
    "    multi_split_dist_corr =multi_split_dist_corr.reindex(natsorted(multi_split_dist_corr.columns), axis=1)\n",
    "#     multi_split_dist_corr.columns = labels\n",
    "    \n",
    "    print(f\"train: {len(x_train)} ({len(x_train)/(len(x_train)+len(x_test)):.2f})\\n\"\n",
    "          f\"test: {len(x_test)} ({len(x_test)/(len(x_train)+len(x_test)):.2f})\")\n",
    "    dist_split = np.mean(np.std(multi_split_dist_corr.to_numpy(), axis=0))\n",
    "    \n",
    "    return dist_split,multi_split_dist,multi_split_dist_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for figure 5, counts for split are in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lt['foldinfo'] = None\n",
    "# display(lt)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# if eighties == True:\n",
    "#     n_split = 10 # 5 for 60-20-20, 10 for 80-10-10\n",
    "#     n_split_in = 9 # 4 for 60-20-20, 9 for 80-10-10\n",
    "# elif eighties == False:\n",
    "n_split = 5 # 5 for 60-20-20, 10 for 80-10-10\n",
    "n_split_in = 4 # 4 for 60-20-20, 9 for 80-10-10\n",
    "\n",
    "fold_taskname = np.empty(shape=(5, 3), dtype=object)\n",
    "\n",
    "# X = np.array([0, 2, 1, 1,0,2,0, 2, 1, 1,0,2,0, 2, 1, 1,0,2])\n",
    "# y = np.array([0, 2, 1, 1,0,2,0, 2, 1, 1,0,2,0, 2, 1, 1,0,2])\n",
    "X = np.array(lt['simplified_diagnosis'].values)\n",
    "y = np.array(lt['simplified_diagnosis'].values)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "## SET UP SPLIT BETWEEN TEST AND TRAIN/VAL\n",
    "skf = StratifiedKFold(n_splits=n_split, random_state=1, shuffle=True)\n",
    "skf.get_n_splits(X, y)\n",
    "print(skf)\n",
    "j = 0\n",
    "for train_val_index, test_index in skf.split(X, y):\n",
    "#     print(\"TRAIN+VAL:\", train_val_index, \"TEST:\", test_index)\n",
    "    ## USE THE GENERATED INDICES TO SELECT DIAGNOSES\n",
    "    q_train_val, q_test = X[train_val_index], X[test_index]\n",
    "    r_train_val, r_test = y[train_val_index], y[test_index]\n",
    "#     print('test:' , test_index)\n",
    "#     print('trainval: ', train_val_index)\n",
    "    skf2 = StratifiedKFold(n_splits=n_split_in, random_state=2, shuffle=True)\n",
    "    skf2.get_n_splits(X, y)\n",
    "    print(skf2)\n",
    "    \n",
    "    ## WITHIN EACH FOLD, SPLIT TRAIN/VAL INTO TRAIN AND VAL (ONLY NEEDED ONCE!)\n",
    "    i = 0\n",
    "    ## example: \n",
    "    ## [0,1,2,3,4,5,6,7,8,9]  full data ['a','b','c','d','e','f','g','h','i','j']\n",
    "    ## [0,2,3,5,7,9] indices selected for train/val ['a','c','d','f','h','j']\n",
    "    ## [0,1,2,3,4,5]\n",
    "    ## [0,2,3,9] indices points selected for train ['a','c','d','j']\n",
    "    for train_index, val_index in skf2.split(q_train_val, r_train_val):\n",
    "        print(i)\n",
    "        if i == 0:\n",
    "            ## USE THE GENERATED INDICES TO CREATE NEW INDICES THAT WORK ON THE FULL DATA\n",
    "            true_train = train_val_index[train_index]\n",
    "            true_val = train_val_index[val_index]\n",
    "            \n",
    "            ## PRINT THE INDICES\n",
    "            print(\"TRAIN:\", true_train, \"\\nVAL:\", true_val, \"\\nTEST:\", test_index)\n",
    "            q_train, q_val, q_test = X[true_train], X[true_val],X[test_index]\n",
    "            r_train, r_val, q_test = y[true_train], y[true_val],y[test_index] \n",
    "            \n",
    "            #print('trainval: ',train_val_index,train_val_index.shape )\n",
    "            #print('train: ',train_index, train_index.shape)\n",
    "            print(f\"train: {len(q_train)} ({len(q_train)/(len(q_train)+len(q_test)+len(q_val)):.2f})\\n\"\n",
    "                  f\"val: {len(q_val)} ({len(q_val)/(len(q_train)+len(q_test)+len(q_val)):.2f})\\n\"\n",
    "                  f\"test: {len(q_test)} ({len(q_test)/(len(q_train)+len(q_test)+len(q_val)):.2f})\")\n",
    "            \n",
    "            ## SAVE INTO NUMPY ARRAY\n",
    "            fold_taskname[j][0] = np.asarray(true_train)\n",
    "            fold_taskname[j][1] = np.asarray(true_val)\n",
    "            fold_taskname[j][2] = np.asarray(test_index)\n",
    "            lt.loc[test_index, 'foldinfo'] = j\n",
    "            \n",
    "            ## FOR VISUALIZING COUNTS PER DIAGNOSIS PER FOLD\n",
    "            ## TRAINING\n",
    "            foo, bar = np.unique(q_train, return_counts=True)\n",
    "            my_dict = dict(zip(foo, bar))\n",
    "            df = pd.DataFrame(list(my_dict.items()),columns = ['diagnosis','train'])\n",
    "            \n",
    "            ## VALIDATION\n",
    "            foo, bar = np.unique(q_val, return_counts=True)\n",
    "            my_dict = dict(zip(foo, bar))\n",
    "            df1 = pd.DataFrame(list(my_dict.items()),columns = ['diagnosis2','val'])\n",
    "            \n",
    "            ## TEST\n",
    "            foo, bar = np.unique(q_test, return_counts=True)\n",
    "            my_dict = dict(zip(foo, bar))\n",
    "            df2 = pd.DataFrame(list(my_dict.items()),columns = ['diagnosis3','test'])\n",
    "            \n",
    "            ## COMBINE ALL THREE\n",
    "            df3 = pd.concat([df,df1, df2], ignore_index=True,axis=1)\n",
    "            df3.columns = ['diagnosis','train','diagnosis2','val','diagnosis3','test']\n",
    "            df3 = df3.drop(['diagnosis2','diagnosis3'], axis=1)\n",
    "            display(df3)\n",
    "            print(df3['diagnosis'])\n",
    "#         elif i > 0:\n",
    "#             print('finished fold {}, exiting...'.format(i))\n",
    "            break\n",
    "        i = i +1\n",
    "    j = j + 1\n",
    "    print('---------')\n",
    "print(fold_taskname)\n",
    "display(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt['foldinfo'].value_counts()\n",
    "lt['indexes'] = lt.index\n",
    "display(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold_taskname[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_input[fold_taskname[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.set_printoptions(threshold=np.inf)\n",
    "n_dim = 87#83\n",
    "mean_taskname = np.zeros((5, 1, n_dim)) * np.nan\n",
    "std_taskname = np.zeros((5, 1, n_dim)) * np.nan\n",
    "for i_split in range(5):\n",
    "    ## fold_taskname[i_split][0] selecteert de indexen van de training donors van elke fold\n",
    "    ## final_input[fold_taskname[i_split][0]] selecteerd de training data van deze donoren\n",
    "    ## de concatenate step combineer het, dus x_tr is training data per fold\n",
    "    x_tr = np.concatenate(final_input[fold_taskname[i_split][0]], axis=0)\n",
    "    display(x_tr)\n",
    "    ## mean taskname contains the mean of each training column. eg. for the first fold, the average age is 75\n",
    "    mean_taskname[i_split][0] = np.nanmean(x_tr, axis=0)\n",
    "    ## std taskname contains the std of each training column. eg. for the first fold, the average age is 12\n",
    "    std_taskname[i_split][0] = np.nanstd(x_tr, axis=0)\n",
    "    \n",
    "print(mean_taskname[0][0])\n",
    "print(std_taskname[0][0])\n",
    "mean_taskname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if eighties == True:\n",
    "#     prefix = '80_'\n",
    "# elif eighties == False:\n",
    "prefix = '60_'\n",
    "savespace = f'clinical_history_{n}_observations'\n",
    "output_path = \"/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_history/temporal_model/data\"    \n",
    "\n",
    "print(savespace)\n",
    "os.makedirs(os.path.join(output_path,  savespace),\n",
    "            exist_ok=True)\n",
    "np.savez(os.path.join(output_path, savespace, 'data.npz'),\n",
    "         input=final_input, masking=final_masking, timestamp=final_timestamp, label_taskname=final_label_taskname)\n",
    "np.savez(os.path.join(output_path, savespace, 'fold.npz'),\n",
    "         fold_taskname=fold_taskname, mean_taskname=mean_taskname, std_taskname=std_taskname)\n",
    "lt.to_excel(os.path.join(output_path, savespace,'donorindexes.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_d.to_csv('/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_diagnosis/output/gru_d_july.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for seurat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### selecting a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_of_choice = 'table1_p' #fig 4a table3_with_con_p #table2_p #fig 3a table1_P fig sup 5a:table2_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "selected_diagnoses,ordered_diagnoses = table_selector(table_of_choice, predictions_df)\n",
    "print('After selecting for {}, we have {} donors'.format(selected_diagnoses['neuropathological_diagnosis'].unique(),\n",
    "                                                                                    selected_diagnoses['DonorID'].nunique()) )\n",
    "display(selected_diagnoses[selected_diagnoses['neuropathological_diagnosis']=='AD'].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge the table1 diagnoses back with the other donors\n",
    "we do this so we can run the analysis on the table1 diagnosis, but also include other groups such as ad dlb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unique_donor_ids = selected_diagnoses['DonorID'].unique().tolist()\n",
    "print(len(unique_donor_ids))\n",
    "# Filter rows from dfa where DonorID is not in the unique_donor_ids list\n",
    "filtered_predictions_df = predictions_df[~predictions_df['DonorID'].isin(unique_donor_ids)]\n",
    "print(filtered_predictions_df.shape)\n",
    "print(predictions_df.shape)\n",
    "# Concatenate filtered_dfa with dfb\n",
    "merged_df = pd.concat([selected_diagnoses, filtered_predictions_df], ignore_index=True)\n",
    "# merged_df['neuropathological_diagnosis'].value_counts().head(40)\n",
    "unique_diagnoses = merged_df[['DonorID', 'neuropathological_diagnosis']].drop_duplicates()\n",
    "unique_diagnoses['neuropathological_diagnosis'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('/home/jupyter-n.mekkes@gmail.com-f6d87/clinical_diagnosis/output/selected_diagnoses_july.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psych = ['MDD', 'SCZ', 'BP']\n",
    "# selected_diagnoses = selected_diagnoses[~selected_diagnoses['neuropathological_diagnosis'].isin(psych)]\n",
    "# ordered_diagnoses =  ['AD', 'PD', 'VD','ATAXIA','DLB','FTD', 'MND', 'PSP', 'MS','MSA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd = merged_df[merged_df['neuropathological_diagnosis'] == 'PD']\n",
    "pad = merged_df.copy()\n",
    "pad['neuropathological_diagnosis'] = pad['neuropathological_diagnosis'].replace('PDD', 'PD')\n",
    "\n",
    "pad = pad[['neuropathological_diagnosis','age_at_death','sex','Constipation','Weight_loss','DonorID','Age']]\n",
    "# pd['Constipation'].value_counts()\n",
    "# Weight loss Constipation\n",
    "pad = pad.groupby(['neuropathological_diagnosis', 'age_at_death', 'sex', 'DonorID']).agg({'Constipation': 'sum', 'Weight_loss': 'sum'}).reset_index()\n",
    "\n",
    "# display(pad)\n",
    "con = pad.groupby('neuropathological_diagnosis')['Constipation'].apply(lambda x: (x > 0).sum()).reset_index(name='Constipation Count')\n",
    "wl = pad.groupby('neuropathological_diagnosis')['Constipation'].apply(lambda x: (x > 0).sum()).reset_index(name='Weight loss Count')\n",
    "\n",
    "# display(con)\n",
    "\n",
    "percentage_df = pad.groupby('neuropathological_diagnosis')[['Constipation','Weight_loss']].apply(lambda x: (x > 0).mean() * 100).reset_index()\n",
    "percentage_df.rename(columns={'Constipation': 'Constipation Percentage','Weight_loss':'Weight_loss Percentage'}, inplace=True)\n",
    "percentage_df['Constipation Count'] = con['Constipation Count']\n",
    "percentage_df['Weight loss Count'] = wl['Weight loss Count']\n",
    "percentage_df.sort_values(by=['Constipation Percentage'],ascending=False,inplace=True)\n",
    "display(percentage_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute_grouping = pd.read_excel(path_to_attribute_grouping, engine='openpyxl', index_col=[0])#,header=3, sheet_name='S3. 90 signs and symptoms')\n",
    "# # display(attribute_grouping.head())\n",
    "# df = predictions_df.copy()\n",
    "\n",
    "# df['symptoms'] = df[attributes].apply(lambda row: ', '.join([col for col in attributes if row[col] != 0]), axis=1)\n",
    "# df.loc[(df[attributes] == 0).all(axis=1), 'symptoms'] = 'none'\n",
    "# columns_to_keep = set(df.columns).difference(attributes)\n",
    "# # df = df[columns_to_keep].copy()\n",
    "# df['symptoms'] = df['symptoms'].str.split(',').apply(lambda x: ', '.join(set(x))).str.strip()\n",
    "\n",
    "# df.tail(10)\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 100)\n",
    "# dfgrouping = predictions_df.copy()\n",
    "\n",
    "# # Iterate over the columns\n",
    "# for column in attributes:\n",
    "#     mask = dfgrouping[column] == 1\n",
    "#     grouping = attribute_grouping.loc[attribute_grouping['ITname'] == column, 'Grouping'].iloc[0]\n",
    "#     dfgrouping.loc[mask, column] = grouping\n",
    "\n",
    "# dfgrouping['groupings'] = dfgrouping[attributes].apply(lambda x: ', '.join([val for val in x if val != 0]), axis=1)\n",
    "# dfgrouping.loc[(dfgrouping[attributes] == 0).all(axis=1), 'groupings'] = 'none'\n",
    "\n",
    "# columns_to_keep = set(dfgrouping.columns).difference(attributes)\n",
    "# dfgrouping = dfgrouping[columns_to_keep].copy()\n",
    "# dfgrouping['groupings'] = dfgrouping['groupings'].str.split(',').apply(lambda x: ', '.join(set([item.strip() for item in x]))).str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# ############ domain #############\n",
    "# dfdomain = predictions_df.copy()\n",
    "# for column in attributes:\n",
    "#     mask = dfdomain[column] == 1\n",
    "#     domain = attribute_grouping.loc[attribute_grouping['ITname'] == column, 'Domain'].iloc[0]\n",
    "#     dfdomain.loc[mask, column] = domain\n",
    "\n",
    "# dfdomain['Domains'] = dfdomain[attributes].apply(lambda x: ', '.join([val for val in x if val != 0]), axis=1)\n",
    "# dfdomain.loc[(dfdomain[attributes] == 0).all(axis=1), 'Domains'] = 'none'\n",
    "\n",
    "# columns_to_keep = set(dfdomain.columns).difference(attributes)\n",
    "# dfdomain = dfdomain[columns_to_keep].copy()\n",
    "# dfdomain['Domains'] = dfdomain['Domains'].str.split(',').apply(lambda x: ', '.join(set([item.strip() for item in x]))).str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# merged_df = df.merge(dfgrouping, on=['neuropathological_diagnosis', 'DonorID', 'Year', 'Age', 'sex', 'age_at_death'])\n",
    "# predictions_df = merged_df.merge(dfdomain, on=['neuropathological_diagnosis', 'DonorID', 'Year', 'Age', 'sex', 'age_at_death'])\n",
    "# display(predictions_df.head(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clinical_history",
   "language": "python",
   "name": "clinical_history"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
